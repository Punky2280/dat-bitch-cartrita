import React, { useState, useRef, useEffect, useCallback } from 'react';\nimport { \n  MicrophoneIcon, \n  StopIcon, \n  SpeakerWaveIcon,\n  EyeIcon,\n  EyeSlashIcon,\n  AdjustmentsHorizontalIcon,\n  SparklesIcon\n} from '@heroicons/react/24/outline';\n\ninterface VoiceChatState {\n  isListening: boolean;\n  isProcessing: boolean;\n  isSpeaking: boolean;\n  voiceActivated: boolean;\n  ambientMode: boolean;\n  visualMode: boolean;\n  sessionActive: boolean;\n}\n\ninterface VoiceChatProps {\n  onTranscript?: (text: string) => void;\n  onResponse?: (response: any) => void;\n  token?: string;\n  userId?: string;\n}\n\nexport const VoiceChatInterface: React.FC<VoiceChatProps> = ({ \n  onTranscript, \n  onResponse,\n  token,\n  userId \n}) => {\n  // State management\n  const [chatState, setChatState] = useState<VoiceChatState>({\n    isListening: false,\n    isProcessing: false,\n    isSpeaking: false,\n    voiceActivated: false,\n    ambientMode: false,\n    visualMode: false,\n    sessionActive: false\n  });\n\n  const [currentTranscript, setCurrentTranscript] = useState<string>('');\n  const [interimTranscript, setInterimTranscript] = useState<string>('');\n  const [lastResponse, setLastResponse] = useState<string>('');\n  const [conversationHistory, setConversationHistory] = useState<any[]>([]);\n  const [visualFeed, setVisualFeed] = useState<string | null>(null);\n  const [ambientSounds, setAmbientSounds] = useState<string[]>([]);\n  const [emotionalState, setEmotionalState] = useState<string>('friendly');\n  const [wakeWordDetected, setWakeWordDetected] = useState<boolean>(false);\n\n  // Refs for media handling\n  const mediaRecorderRef = useRef<MediaRecorder | null>(null);\n  const streamRef = useRef<MediaStream | null>(null);\n  const audioChunksRef = useRef<Blob[]>([]);\n  const videoRef = useRef<HTMLVideoElement>(null);\n  const canvasRef = useRef<HTMLCanvasElement>(null);\n  const audioContextRef = useRef<AudioContext | null>(null);\n  const analyserRef = useRef<AnalyserNode | null>(null);\n\n  // Real-time audio analysis\n  const setupAudioAnalysis = useCallback(async () => {\n    try {\n      if (!streamRef.current) return;\n\n      audioContextRef.current = new AudioContext();\n      analyserRef.current = audioContextRef.current.createAnalyser();\n      \n      const source = audioContextRef.current.createMediaStreamSource(streamRef.current);\n      source.connect(analyserRef.current);\n      \n      analyserRef.current.fftSize = 256;\n      const bufferLength = analyserRef.current.frequencyBinCount;\n      const dataArray = new Uint8Array(bufferLength);\n\n      const analyzeAudio = () => {\n        if (!analyserRef.current || !chatState.sessionActive) return;\n        \n        analyserRef.current.getByteFrequencyData(dataArray);\n        \n        // Calculate volume level\n        const average = dataArray.reduce((sum, value) => sum + value, 0) / bufferLength;\n        const volumeLevel = average / 255;\n        \n        // Detect speech patterns\n        if (volumeLevel > 0.1 && !chatState.isProcessing) {\n          // Speech detected - could trigger wake word detection\n          if (!chatState.voiceActivated && volumeLevel > 0.3) {\n            checkForWakeWord();\n          }\n        }\n        \n        requestAnimationFrame(analyzeAudio);\n      };\n      \n      analyzeAudio();\n    } catch (error) {\n      console.error('[VoiceChat] Error setting up audio analysis:', error);\n    }\n  }, [chatState.sessionActive, chatState.voiceActivated, chatState.isProcessing]);\n\n  // Wake word detection\n  const checkForWakeWord = useCallback(async () => {\n    if (chatState.voiceActivated || !audioChunksRef.current.length) return;\n    \n    try {\n      // Create a short audio clip for wake word detection\n      const audioBlob = new Blob(audioChunksRef.current.slice(-2), { type: 'audio/webm' });\n      \n      if (audioBlob.size < 1000) return; // Too small to analyze\n      \n      const formData = new FormData();\n      formData.append('audio', audioBlob, 'wake-check.webm');\n      \n      const response = await fetch('/api/voice-to-text/transcribe', {\n        method: 'POST',\n        headers: {\n          Authorization: `Bearer ${token}`,\n        },\n        body: formData,\n      });\n      \n      if (response.ok) {\n        const result = await response.json();\n        \n        if (result.wakeWord && result.wakeWord.detected) {\n          console.log('[VoiceChat] Wake word detected:', result.wakeWord.wakeWord);\n          setWakeWordDetected(true);\n          activateVoiceMode(result.wakeWord.cleanTranscript);\n        }\n      }\n    } catch (error) {\n      console.error('[VoiceChat] Wake word detection error:', error);\n    }\n  }, [chatState.voiceActivated, token]);\n\n  // Activate voice mode\n  const activateVoiceMode = useCallback(async (initialCommand?: string) => {\n    try {\n      console.log('[VoiceChat] Activating voice mode');\n      \n      setChatState(prev => ({ ...prev, voiceActivated: true, isListening: true }));\n      setWakeWordDetected(true);\n      \n      // Send acknowledgment\n      const acknowledgment = \"Hey! I'm here, what's up?\";\n      await playTTSResponse(acknowledgment, 'friendly');\n      \n      // Process initial command if provided\n      if (initialCommand && initialCommand.trim()) {\n        setTimeout(() => {\n          processVoiceInput(initialCommand);\n        }, 1500);\n      }\n      \n    } catch (error) {\n      console.error('[VoiceChat] Error activating voice mode:', error);\n    }\n  }, []);\n\n  // Start comprehensive session\n  const startVoiceSession = useCallback(async () => {\n    try {\n      if (chatState.sessionActive) {\n        console.warn('[VoiceChat] Session already active');\n        return;\n      }\n\n      console.log('[VoiceChat] Starting comprehensive voice session');\n      \n      // Get media permissions\n      const stream = await navigator.mediaDevices.getUserMedia({\n        audio: {\n          echoCancellation: true,\n          noiseSuppression: true,\n          sampleRate: 16000,\n        },\n        video: chatState.visualMode ? {\n          width: { ideal: 1280 },\n          height: { ideal: 720 },\n          frameRate: { ideal: 15 }\n        } : false\n      });\n\n      streamRef.current = stream;\n      \n      // Set up video if enabled\n      if (chatState.visualMode && videoRef.current) {\n        videoRef.current.srcObject = stream;\n      }\n      \n      // Set up audio recording\n      audioChunksRef.current = [];\n      const mediaRecorder = new MediaRecorder(stream, {\n        mimeType: 'audio/webm',\n      });\n      \n      mediaRecorderRef.current = mediaRecorder;\n      \n      mediaRecorder.ondataavailable = event => {\n        if (event.data.size > 0) {\n          audioChunksRef.current.push(event.data);\n          \n          // Keep only recent chunks for wake word detection\n          if (audioChunksRef.current.length > 10) {\n            audioChunksRef.current = audioChunksRef.current.slice(-10);\n          }\n        }\n      };\n      \n      mediaRecorder.start(500); // Collect data every 500ms\n      \n      // Set up audio analysis\n      await setupAudioAnalysis();\n      \n      // Start backend voice session\n      const sessionResponse = await fetch('/api/voice-chat/start-session', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          Authorization: `Bearer ${token}`,\n        },\n        body: JSON.stringify({\n          settings: {\n            wakeWords: ['cartrita', 'hey cartrita', 'cartrita!'],\n            ambientMode: chatState.ambientMode,\n            visualMode: chatState.visualMode,\n            sensitivity: 0.3\n          }\n        })\n      });\n      \n      if (sessionResponse.ok) {\n        setChatState(prev => ({ \n          ...prev, \n          sessionActive: true, \n          isListening: true \n        }));\n        \n        // Start visual analysis if enabled\n        if (chatState.visualMode) {\n          startVisualAnalysis();\n        }\n        \n        // Start ambient monitoring if enabled\n        if (chatState.ambientMode) {\n          startAmbientMonitoring();\n        }\n        \n        // Play greeting\n        await playTTSResponse(\"Hey! I'm all set up and ready to chat! Just say my name when you want to talk!\", 'friendly');\n      }\n      \n    } catch (error) {\n      console.error('[VoiceChat] Failed to start session:', error);\n      alert('Failed to start voice chat. Please check your microphone permissions.');\n    }\n  }, [chatState.sessionActive, chatState.visualMode, chatState.ambientMode, token, setupAudioAnalysis]);\n\n  // Stop session\n  const stopVoiceSession = useCallback(async () => {\n    try {\n      console.log('[VoiceChat] Stopping voice session');\n      \n      // Stop media recorder\n      if (mediaRecorderRef.current && mediaRecorderRef.current.state !== 'inactive') {\n        mediaRecorderRef.current.stop();\n      }\n      \n      // Stop media stream\n      if (streamRef.current) {\n        streamRef.current.getTracks().forEach(track => track.stop());\n        streamRef.current = null;\n      }\n      \n      // Stop audio context\n      if (audioContextRef.current) {\n        audioContextRef.current.close();\n        audioContextRef.current = null;\n      }\n      \n      // Stop backend session\n      await fetch('/api/voice-chat/stop-session', {\n        method: 'POST',\n        headers: {\n          Authorization: `Bearer ${token}`,\n        }\n      });\n      \n      setChatState({\n        isListening: false,\n        isProcessing: false,\n        isSpeaking: false,\n        voiceActivated: false,\n        ambientMode: false,\n        visualMode: false,\n        sessionActive: false\n      });\n      \n      setWakeWordDetected(false);\n      setCurrentTranscript('');\n      setInterimTranscript('');\n      \n    } catch (error) {\n      console.error('[VoiceChat] Error stopping session:', error);\n    }\n  }, [token]);\n\n  // Process voice input\n  const processVoiceInput = useCallback(async (transcript: string) => {\n    try {\n      setChatState(prev => ({ ...prev, isProcessing: true }));\n      setCurrentTranscript(transcript);\n      \n      // Add to conversation history\n      const userMessage = {\n        timestamp: new Date(),\n        type: 'user',\n        content: transcript,\n        mode: 'voice'\n      };\n      \n      setConversationHistory(prev => [...prev, userMessage]);\n      \n      if (onTranscript) {\n        onTranscript(transcript);\n      }\n      \n      // Get response from backend (this would integrate with your agent system)\n      const response = await generateVoiceResponse(transcript);\n      \n      // Add response to history\n      const assistantMessage = {\n        timestamp: new Date(),\n        type: 'assistant',\n        content: response.text,\n        emotion: response.emotion,\n        mode: 'voice'\n      };\n      \n      setConversationHistory(prev => [...prev, assistantMessage]);\n      setLastResponse(response.text);\n      setEmotionalState(response.emotion);\n      \n      // Play TTS response\n      await playTTSResponse(response.text, response.emotion);\n      \n      if (onResponse) {\n        onResponse(response);\n      }\n      \n    } catch (error) {\n      console.error('[VoiceChat] Error processing voice input:', error);\n      await playTTSResponse(\"Sorry, I had trouble with that. Can you try again?\", 'calm');\n    } finally {\n      setChatState(prev => ({ ...prev, isProcessing: false }));\n    }\n  }, [onTranscript, onResponse]);\n\n  // Generate voice response\n  const generateVoiceResponse = useCallback(async (input: string) => {\n    // This would integrate with your full agent system\n    // For now, using simple responses\n    const responses = {\n      greeting: {\n        patterns: ['hello', 'hi', 'hey', 'good morning', 'good afternoon'],\n        response: \"Hey there! So good to hear your voice! What's going on today?\",\n        emotion: 'friendly'\n      },\n      compliment: {\n        patterns: ['beautiful', 'amazing', 'great', 'awesome', 'incredible'],\n        response: \"Aww, you're so sweet! Thank you, that totally made my day!\",\n        emotion: 'excited'\n      },\n      question: {\n        patterns: ['what', 'how', 'why', 'when', 'where', 'can you'],\n        response: \"That's a great question! I love talking about stuff like this. Let me think...\",\n        emotion: 'curious'\n      },\n      help: {\n        patterns: ['help', 'assist', 'support'],\n        response: \"Of course I can help! I'm here for whatever you need. What can I do for you?\",\n        emotion: 'encouraging'\n      }\n    };\n    \n    const lowerInput = input.toLowerCase();\n    \n    for (const [category, config] of Object.entries(responses)) {\n      if (config.patterns.some(pattern => lowerInput.includes(pattern))) {\n        return {\n          text: config.response,\n          emotion: config.emotion,\n          category: category\n        };\n      }\n    }\n    \n    // Default response\n    return {\n      text: \"That's really interesting! I love hearing your thoughts. Tell me more about that!\",\n      emotion: 'friendly',\n      category: 'default'\n    };\n  }, []);\n\n  // Play TTS response\n  const playTTSResponse = useCallback(async (text: string, emotion: string = 'friendly') => {\n    try {\n      setChatState(prev => ({ ...prev, isSpeaking: true }));\n      \n      const response = await fetch('/api/voice-chat/speak', {\n        method: 'POST',\n        headers: {\n          'Content-Type': 'application/json',\n          Authorization: `Bearer ${token}`,\n        },\n        body: JSON.stringify({\n          text: text,\n          emotion: emotion,\n          voice: 'nova',\n          speed: 1.0\n        })\n      });\n      \n      if (response.ok) {\n        const audioBuffer = await response.arrayBuffer();\n        const audioContext = new AudioContext();\n        const audioData = await audioContext.decodeAudioData(audioBuffer);\n        const source = audioContext.createBufferSource();\n        \n        source.buffer = audioData;\n        source.connect(audioContext.destination);\n        \n        source.onended = () => {\n          setChatState(prev => ({ ...prev, isSpeaking: false }));\n        };\n        \n        source.start();\n      }\n      \n    } catch (error) {\n      console.error('[VoiceChat] Error playing TTS response:', error);\n      setChatState(prev => ({ ...prev, isSpeaking: false }));\n    }\n  }, [token]);\n\n  // Visual analysis\n  const startVisualAnalysis = useCallback(async () => {\n    if (!videoRef.current || !canvasRef.current) return;\n    \n    const analyzeFrame = async () => {\n      if (!chatState.visualMode || !chatState.sessionActive) return;\n      \n      const canvas = canvasRef.current!;\n      const context = canvas.getContext('2d')!;\n      const video = videoRef.current!;\n      \n      if (video.readyState >= 2) {\n        canvas.width = video.videoWidth;\n        canvas.height = video.videoHeight;\n        context.drawImage(video, 0, 0);\n        \n        // Convert to blob and send for analysis\n        canvas.toBlob(async (blob) => {\n          if (!blob) return;\n          \n          const formData = new FormData();\n          formData.append('image', blob, 'frame.jpg');\n          \n          try {\n            const response = await fetch('/api/vision/analyze', {\n              method: 'POST',\n              headers: {\n                Authorization: `Bearer ${token}`,\n              },\n              body: formData\n            });\n            \n            if (response.ok) {\n              const analysis = await response.json();\n              // Process visual analysis results\n              processVisualAnalysis(analysis);\n            }\n          } catch (error) {\n            console.error('[VoiceChat] Visual analysis error:', error);\n          }\n        }, 'image/jpeg', 0.8);\n      }\n      \n      // Analyze every 5 seconds\n      setTimeout(analyzeFrame, 5000);\n    };\n    \n    setTimeout(analyzeFrame, 2000); // Start after 2 seconds\n  }, [chatState.visualMode, chatState.sessionActive, token]);\n\n  // Process visual analysis\n  const processVisualAnalysis = useCallback((analysis: any) => {\n    if (analysis.cartrita_comments && analysis.cartrita_comments.length > 0) {\n      // Occasionally comment on what she sees (10% chance)\n      if (Math.random() < 0.1) {\n        const comment = analysis.cartrita_comments[0];\n        setTimeout(() => {\n          playTTSResponse(comment, 'casual');\n        }, 1000);\n      }\n    }\n  }, [playTTSResponse]);\n\n  // Ambient monitoring\n  const startAmbientMonitoring = useCallback(() => {\n    // This would connect to ambient listening service\n    console.log('[VoiceChat] Starting ambient monitoring');\n  }, []);\n\n  // Toggle functions\n  const toggleAmbientMode = useCallback(() => {\n    setChatState(prev => ({ ...prev, ambientMode: !prev.ambientMode }));\n  }, []);\n\n  const toggleVisualMode = useCallback(() => {\n    setChatState(prev => ({ ...prev, visualMode: !prev.visualMode }));\n  }, []);\n\n  const toggleSession = useCallback(() => {\n    if (chatState.sessionActive) {\n      stopVoiceSession();\n    } else {\n      startVoiceSession();\n    }\n  }, [chatState.sessionActive, startVoiceSession, stopVoiceSession]);\n\n  // Cleanup on unmount\n  useEffect(() => {\n    return () => {\n      if (chatState.sessionActive) {\n        stopVoiceSession();\n      }\n    };\n  }, []);\n\n  return (\n    <div className=\"voice-chat-interface bg-gray-50 dark:bg-gray-900 rounded-lg p-6 shadow-lg\">\n      {/* Header */}\n      <div className=\"flex items-center justify-between mb-6\">\n        <div className=\"flex items-center space-x-3\">\n          <SparklesIcon className=\"h-6 w-6 text-purple-500\" />\n          <h2 className=\"text-xl font-bold text-gray-900 dark:text-white\">\n            Cartrita Voice Chat\n          </h2>\n        </div>\n        \n        <div className=\"flex items-center space-x-2\">\n          {/* Visual Mode Toggle */}\n          <button\n            onClick={toggleVisualMode}\n            className={`p-2 rounded-full transition-colors ${\n              chatState.visualMode\n                ? 'bg-green-500 text-white'\n                : 'bg-gray-200 text-gray-600 dark:bg-gray-700 dark:text-gray-300'\n            }`}\n            title=\"Toggle Visual Mode\"\n          >\n            {chatState.visualMode ? <EyeIcon className=\"h-5 w-5\" /> : <EyeSlashIcon className=\"h-5 w-5\" />}\n          </button>\n          \n          {/* Ambient Mode Toggle */}\n          <button\n            onClick={toggleAmbientMode}\n            className={`p-2 rounded-full transition-colors ${\n              chatState.ambientMode\n                ? 'bg-blue-500 text-white'\n                : 'bg-gray-200 text-gray-600 dark:bg-gray-700 dark:text-gray-300'\n            }`}\n            title=\"Toggle Ambient Listening\"\n          >\n            <AdjustmentsHorizontalIcon className=\"h-5 w-5\" />\n          </button>\n        </div>\n      </div>\n\n      {/* Status Display */}\n      <div className=\"mb-6\">\n        <div className=\"flex items-center space-x-4 text-sm\">\n          <div className={`flex items-center space-x-2 ${\n            chatState.sessionActive ? 'text-green-600' : 'text-gray-500'\n          }`}>\n            <div className={`w-2 h-2 rounded-full ${\n              chatState.sessionActive ? 'bg-green-500 animate-pulse' : 'bg-gray-400'\n            }`} />\n            <span>{chatState.sessionActive ? 'Active' : 'Inactive'}</span>\n          </div>\n          \n          {wakeWordDetected && (\n            <div className=\"flex items-center space-x-2 text-purple-600\">\n              <SparklesIcon className=\"h-4 w-4\" />\n              <span>Voice Activated</span>\n            </div>\n          )}\n          \n          {chatState.isSpeaking && (\n            <div className=\"flex items-center space-x-2 text-blue-600\">\n              <SpeakerWaveIcon className=\"h-4 w-4 animate-pulse\" />\n              <span>Speaking</span>\n            </div>\n          )}\n        </div>\n      </div>\n\n      {/* Visual Feed */}\n      {chatState.visualMode && (\n        <div className=\"mb-6\">\n          <video\n            ref={videoRef}\n            autoPlay\n            muted\n            className=\"w-full max-w-md mx-auto rounded-lg shadow-md\"\n            style={{ maxHeight: '200px' }}\n          />\n          <canvas ref={canvasRef} style={{ display: 'none' }} />\n        </div>\n      )}\n\n      {/* Conversation Display */}\n      <div className=\"mb-6 h-64 overflow-y-auto bg-white dark:bg-gray-800 rounded-lg p-4 border\">\n        {conversationHistory.length === 0 ? (\n          <div className=\"text-center text-gray-500 dark:text-gray-400 py-8\">\n            <SparklesIcon className=\"h-8 w-8 mx-auto mb-2 opacity-50\" />\n            <p>Start chatting with Cartrita!</p>\n            <p className=\"text-sm mt-1\">Say \"Cartrita!\" to activate voice mode</p>\n          </div>\n        ) : (\n          <div className=\"space-y-4\">\n            {conversationHistory.map((message, index) => (\n              <div\n                key={index}\n                className={`flex ${\n                  message.type === 'user' ? 'justify-end' : 'justify-start'\n                }`}\n              >\n                <div\n                  className={`max-w-xs lg:max-w-md px-4 py-2 rounded-lg ${\n                    message.type === 'user'\n                      ? 'bg-blue-500 text-white'\n                      : 'bg-gray-200 dark:bg-gray-700 text-gray-900 dark:text-white'\n                  }`}\n                >\n                  <p>{message.content}</p>\n                  {message.emotion && (\n                    <p className=\"text-xs opacity-75 mt-1\">({message.emotion})</p>\n                  )}\n                </div>\n              </div>\n            ))}\n          </div>\n        )}\n        \n        {/* Interim Transcript */}\n        {interimTranscript && (\n          <div className=\"flex justify-end mt-2\">\n            <div className=\"max-w-xs lg:max-w-md px-4 py-2 rounded-lg bg-blue-300 text-blue-900 opacity-75\">\n              <p className=\"italic\">{interimTranscript}...</p>\n            </div>\n          </div>\n        )}\n      </div>\n\n      {/* Controls */}\n      <div className=\"flex items-center justify-center space-x-4\">\n        {/* Main Voice Button */}\n        <button\n          onClick={toggleSession}\n          disabled={chatState.isProcessing}\n          className={`\n            relative p-4 rounded-full transition-all duration-200 focus:outline-none focus:ring-4 focus:ring-offset-2\n            ${\n              chatState.sessionActive\n                ? chatState.voiceActivated\n                  ? 'bg-purple-500 hover:bg-purple-600 text-white focus:ring-purple-500 shadow-lg'\n                  : 'bg-green-500 hover:bg-green-600 text-white focus:ring-green-500'\n                : 'bg-blue-500 hover:bg-blue-600 text-white focus:ring-blue-500'\n            }\n            ${\n              chatState.isProcessing\n                ? 'opacity-50 cursor-not-allowed'\n                : 'cursor-pointer transform hover:scale-105'\n            }\n          `}\n          title={\n            chatState.sessionActive\n              ? 'Stop Voice Session'\n              : 'Start Voice Session'\n          }\n        >\n          {chatState.isProcessing ? (\n            <div className=\"animate-spin h-8 w-8 border-2 border-white border-t-transparent rounded-full\" />\n          ) : chatState.sessionActive ? (\n            <StopIcon className=\"h-8 w-8\" />\n          ) : (\n            <MicrophoneIcon className=\"h-8 w-8\" />\n          )}\n          \n          {/* Voice Activated Indicator */}\n          {chatState.voiceActivated && (\n            <div className=\"absolute -top-1 -right-1 w-4 h-4 bg-purple-400 rounded-full animate-pulse\" />\n          )}\n          \n          {/* Listening Indicator */}\n          {chatState.isListening && !chatState.voiceActivated && (\n            <div className=\"absolute -top-1 -right-1 w-4 h-4 bg-green-400 rounded-full animate-pulse\" />\n          )}\n        </button>\n      </div>\n\n      {/* Wake Word Instruction */}\n      {chatState.sessionActive && !chatState.voiceActivated && (\n        <div className=\"mt-4 text-center\">\n          <p className=\"text-sm text-gray-600 dark:text-gray-400\">\n            ðŸ’¡ Say <strong>\"Cartrita!\"</strong> to activate voice mode\n          </p>\n        </div>\n      )}\n\n      {/* Ambient Sounds Display */}\n      {chatState.ambientMode && ambientSounds.length > 0 && (\n        <div className=\"mt-4 p-3 bg-blue-50 dark:bg-blue-900/20 rounded-lg\">\n          <p className=\"text-sm font-medium text-blue-900 dark:text-blue-100 mb-2\">\n            Ambient Sounds Detected:\n          </p>\n          <div className=\"flex flex-wrap gap-2\">\n            {ambientSounds.map((sound, index) => (\n              <span\n                key={index}\n                className=\"px-2 py-1 bg-blue-200 dark:bg-blue-800 text-blue-800 dark:text-blue-200 text-xs rounded-full\"\n              >\n                {sound}\n              </span>\n            ))}\n          </div>\n        </div>\n      )}\n\n      {/* Current Emotional State */}\n      <div className=\"mt-4 text-center\">\n        <p className=\"text-xs text-gray-500 dark:text-gray-400\">\n          Current mood: <span className=\"font-medium text-purple-600 dark:text-purple-400\">{emotionalState}</span>\n        </p>\n      </div>\n    </div>\n  );\n};\n\nexport default VoiceChatInterface;