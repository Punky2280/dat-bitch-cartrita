{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "079de550",
   "metadata": {},
   "source": [
    "# UI/UX Effectiveness Analysis & Optimization\n",
    "\n",
    "## Current Status: January 27, 2025\n",
    "**Objective**: Prepare all UI/UX functionality to be 100% effective by definition\n",
    "\n",
    "### Key Areas Requiring 100% Effectiveness:\n",
    "1. **Real-time System Performance**: All metrics must update in real-time without mock data\n",
    "2. **Agent Integration**: 15 sophisticated agents must be seamlessly integrated into UI\n",
    "3. **Responsive Design**: Mobile-first approach with perfect cross-device compatibility\n",
    "4. **Error Handling**: Graceful error states with user-friendly messaging\n",
    "5. **State Management**: Bulletproof state consistency across all components\n",
    "6. **WebSocket Integration**: Real-time communication without connection drops\n",
    "7. **Authentication Flow**: Seamless login/logout with token management\n",
    "8. **Theme System**: Consistent theming across all components and pages\n",
    "9. **Accessibility**: WCAG compliance for all interactive elements\n",
    "10. **Performance**: Sub-2s load times and smooth 60fps interactions\n",
    "\n",
    "### Current UI/UX Architecture Analysis:\n",
    "- ‚úÖ **App Context**: Comprehensive state management with proper actions\n",
    "- ‚úÖ **Component Structure**: Modern React 18 with TypeScript\n",
    "- ‚úÖ **Theme System**: Multiple theme support (dark, light, cyberpunk, neon, minimal)\n",
    "- ‚úÖ **Error Boundaries**: Proper error handling infrastructure\n",
    "- ‚úÖ **WebSocket Integration**: Real-time communication setup\n",
    "- ‚ö†Ô∏è **Agent Integration**: Need to connect 15 new sophisticated agents\n",
    "- ‚ö†Ô∏è **Performance**: Need real-time metrics instead of mock data\n",
    "- ‚ö†Ô∏è **Mobile Responsiveness**: Need comprehensive mobile optimization\n",
    "\n",
    "### Implementation Strategy:\n",
    "1. **Python Analysis Script**: Analyze current UI/UX effectiveness metrics\n",
    "2. **Real-time Data Integration**: Replace all mock data with live backend calls\n",
    "3. **Agent UI Integration**: Create sophisticated agent interaction interfaces\n",
    "4. **Performance Optimization**: Implement lazy loading and code splitting\n",
    "5. **Mobile-first Redesign**: Ensure perfect mobile experience\n",
    "6. **Accessibility Audit**: WCAG 2.1 AA compliance verification\n",
    "7. **Error State Optimization**: Comprehensive error handling UX\n",
    "8. **WebSocket Reliability**: Connection resilience and auto-reconnect\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "328f5400",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting Cartrita UI/UX Effectiveness Analysis...\n",
      "üîç Starting Cartrita UI/UX Effectiveness Analysis...\n",
      "\n",
      "üéØ CARTRITA UI/UX EFFECTIVENESS REPORT\n",
      "==================================================\n",
      "\n",
      "üìä OVERALL EFFECTIVENESS: 97.5/100\n",
      "üéØ TARGET EFFECTIVENESS: 100/100\n",
      "üìâ EFFECTIVENESS GAP: 2.5 points\n",
      "\n",
      "üìà METRICS BREAKDOWN:\n",
      "\n",
      "üü¢ Component Architecture: 105.0/100 (EXCELLENT)\n",
      "   Issues: 0 | Recommendations: 0\n",
      "\n",
      "üü¢ State Management: 120.0/100 (EXCELLENT)\n",
      "   Issues: 0 | Recommendations: 0\n",
      "\n",
      "üü° Real-time Integration: 85.0/100 (GOOD)\n",
      "   Issues: 1 | Recommendations: 1\n",
      "\n",
      "üü¢ Error Handling: 100.0/100 (EXCELLENT)\n",
      "   Issues: 0 | Recommendations: 0\n",
      "\n",
      "üü¢ Responsive Design: 100.0/100 (EXCELLENT)\n",
      "   Issues: 0 | Recommendations: 0\n",
      "\n",
      "üü¢ Accessibility: 90.0/100 (EXCELLENT)\n",
      "   Issues: 0 | Recommendations: 0\n",
      "\n",
      "üü¢ Performance: 105.0/100 (EXCELLENT)\n",
      "   Issues: 0 | Recommendations: 0\n",
      "\n",
      "üü† Agent Integration: 70.0/100 (NEEDS_IMPROVEMENT)\n",
      "   Issues: 1 | Recommendations: 3\n",
      "\n",
      "üü¢ Theme Consistency: 100.0/100 (EXCELLENT)\n",
      "   Issues: 0 | Recommendations: 0\n",
      "\n",
      "üü¢ WebSocket Integration: 100.0/100 (EXCELLENT)\n",
      "   Issues: 0 | Recommendations: 0\n",
      "\n",
      "üö® CRITICAL ISSUES: 0\n",
      "\n",
      "üîß TOP PRIORITY RECOMMENDATIONS:\n",
      "   1. ‚ö° Integrate 15 sophisticated agents into UI (Agent Integration)\n",
      "   2. ‚ö° Add real-time agent communication (Agent Integration)\n",
      "   3. ‚ö° Create agent management interfaces (Agent Integration)\n",
      "   4. üí° Replace all mock data with real-time API calls (Real-time Integration)\n",
      "\n",
      "‚è±Ô∏è IMPLEMENTATION TIMELINE:\n",
      "   Total Estimated Time: 2 days (0.4 weeks)\n",
      "   Critical Issues: 0 (0 days)\n",
      "   Improvements: 1 (2 days)\n",
      "\n",
      "üéØ NEXT STEPS:\n",
      "   1. Address critical UI/UX issues immediately\n",
      "   2. Implement real-time agent integration\n",
      "   3. Replace mock data with live backend calls\n",
      "   4. Optimize mobile responsiveness\n",
      "   5. Add comprehensive error handling\n",
      "   \n",
      "üìã EFFECTIVENESS STATUS: EXCELLENT\n",
      "\n",
      "üîç Starting Cartrita UI/UX Effectiveness Analysis...\n",
      "\n",
      "üíæ Detailed analysis saved to: /tmp/cartrita_ui_effectiveness_analysis.json\n",
      "‚è±Ô∏è Analysis completed in: 0.08s\n",
      "\n",
      "üéØ Ready to implement 100% UI/UX effectiveness!\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Cartrita UI/UX Effectiveness Analysis & Optimization Script\n",
    "Analyzes current frontend for 100% effectiveness and generates improvement recommendations\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import time\n",
    "\n",
    "@dataclass\n",
    "class UIEffectivenessMetric:\n",
    "    \"\"\"Metric for measuring UI/UX effectiveness\"\"\"\n",
    "    name: str\n",
    "    current_score: float  # 0-100\n",
    "    target_score: float   # Should be 100 for effectiveness\n",
    "    status: str           # 'excellent', 'good', 'needs_improvement', 'critical'\n",
    "    issues: List[str]\n",
    "    recommendations: List[str]\n",
    "    \n",
    "class CartritaUIAnalyzer:\n",
    "    \"\"\"Comprehensive UI/UX effectiveness analyzer\"\"\"\n",
    "    \n",
    "    def __init__(self, frontend_path: str = \"/home/robbie/development/dat-bitch-cartrita/packages/frontend\"):\n",
    "        self.frontend_path = Path(frontend_path)\n",
    "        self.src_path = self.frontend_path / \"src\"\n",
    "        self.metrics: List[UIEffectivenessMetric] = []\n",
    "        \n",
    "    def analyze_all_metrics(self) -> Dict[str, Any]:\n",
    "        \"\"\"Perform comprehensive UI/UX effectiveness analysis\"\"\"\n",
    "        print(\"üîç Starting Cartrita UI/UX Effectiveness Analysis...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Core effectiveness metrics\n",
    "        self._analyze_component_architecture()\n",
    "        self._analyze_state_management()\n",
    "        self._analyze_real_time_integration()\n",
    "        self._analyze_error_handling()\n",
    "        self._analyze_responsive_design()\n",
    "        self._analyze_accessibility()\n",
    "        self._analyze_performance()\n",
    "        self._analyze_agent_integration()\n",
    "        self._analyze_theme_consistency()\n",
    "        self._analyze_websocket_integration()\n",
    "        \n",
    "        # Calculate overall effectiveness score\n",
    "        overall_score = sum(m.current_score for m in self.metrics) / len(self.metrics)\n",
    "        analysis_time = time.time() - start_time\n",
    "        \n",
    "        return {\n",
    "            \"overall_effectiveness\": overall_score,\n",
    "            \"target_effectiveness\": 100.0,\n",
    "            \"effectiveness_gap\": 100.0 - overall_score,\n",
    "            \"metrics\": [asdict(m) for m in self.metrics],\n",
    "            \"critical_issues\": [m for m in self.metrics if m.status == 'critical'],\n",
    "            \"analysis_duration\": f\"{analysis_time:.2f}s\",\n",
    "            \"recommendations\": self._generate_prioritized_recommendations(),\n",
    "            \"implementation_timeline\": self._estimate_implementation_timeline()\n",
    "        }\n",
    "    \n",
    "    def _analyze_component_architecture(self):\n",
    "        \"\"\"Analyze component architecture effectiveness\"\"\"\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "        score = 85.0  # Good baseline\n",
    "        \n",
    "        # Check component structure\n",
    "        components_path = self.src_path / \"components\"\n",
    "        pages_path = self.src_path / \"pages\"\n",
    "        \n",
    "        if not components_path.exists():\n",
    "            issues.append(\"Components directory not found\")\n",
    "            score -= 20\n",
    "        else:\n",
    "            # Count component files\n",
    "            component_files = list(components_path.glob(\"**/*.tsx\"))\n",
    "            if len(component_files) < 10:\n",
    "                issues.append(f\"Only {len(component_files)} components found - may need more modular architecture\")\n",
    "                score -= 10\n",
    "            \n",
    "        # Check for modern patterns\n",
    "        app_tsx = self.src_path / \"App.tsx\"\n",
    "        if app_tsx.exists():\n",
    "            content = app_tsx.read_text()\n",
    "            if \"React.FC\" not in content and \"function\" in content:\n",
    "                score += 5  # Good functional components\n",
    "            if \"useState\" in content and \"useEffect\" in content:\n",
    "                score += 5  # Good hooks usage\n",
    "        \n",
    "        # Check for TypeScript usage\n",
    "        tsx_files = list(self.src_path.glob(\"**/*.tsx\"))\n",
    "        ts_files = list(self.src_path.glob(\"**/*.ts\"))\n",
    "        total_files = len(tsx_files) + len(ts_files)\n",
    "        \n",
    "        if total_files > 0:\n",
    "            score += 10  # TypeScript bonus\n",
    "        else:\n",
    "            issues.append(\"No TypeScript files detected\")\n",
    "            score -= 15\n",
    "            \n",
    "        status = self._get_status_from_score(score)\n",
    "        \n",
    "        if score < 90:\n",
    "            recommendations.append(\"Implement more modular component architecture\")\n",
    "        if score < 80:\n",
    "            recommendations.append(\"Add comprehensive TypeScript types\")\n",
    "            \n",
    "        self.metrics.append(UIEffectivenessMetric(\n",
    "            name=\"Component Architecture\",\n",
    "            current_score=score,\n",
    "            target_score=100.0,\n",
    "            status=status,\n",
    "            issues=issues,\n",
    "            recommendations=recommendations\n",
    "        ))\n",
    "    \n",
    "    def _analyze_state_management(self):\n",
    "        \"\"\"Analyze state management effectiveness\"\"\"\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "        score = 70.0\n",
    "        \n",
    "        # Check for context providers\n",
    "        app_context = self.src_path / \"context\" / \"AppContext.tsx\"\n",
    "        if app_context.exists():\n",
    "            score += 20\n",
    "            content = app_context.read_text()\n",
    "            \n",
    "            # Check for useReducer pattern\n",
    "            if \"useReducer\" in content:\n",
    "                score += 10\n",
    "            else:\n",
    "                issues.append(\"Not using useReducer for complex state\")\n",
    "                score -= 5\n",
    "                \n",
    "            # Check for comprehensive actions\n",
    "            if \"dispatch\" in content and \"actions\" in content:\n",
    "                score += 10\n",
    "            \n",
    "            # Check for async actions\n",
    "            if \"async\" in content and \"Promise\" in content:\n",
    "                score += 5\n",
    "        else:\n",
    "            issues.append(\"AppContext not found - centralized state management missing\")\n",
    "            score -= 30\n",
    "            \n",
    "        # Check for other state management\n",
    "        context_files = list((self.src_path / \"context\").glob(\"*.tsx\")) if (self.src_path / \"context\").exists() else []\n",
    "        if len(context_files) > 1:\n",
    "            score += 5  # Multiple contexts\n",
    "            \n",
    "        status = self._get_status_from_score(score)\n",
    "        \n",
    "        if score < 85:\n",
    "            recommendations.append(\"Implement comprehensive state management with useReducer\")\n",
    "        if score < 70:\n",
    "            recommendations.append(\"Add proper async action creators\")\n",
    "            \n",
    "        self.metrics.append(UIEffectivenessMetric(\n",
    "            name=\"State Management\",\n",
    "            current_score=score,\n",
    "            target_score=100.0,\n",
    "            status=status,\n",
    "            issues=issues,\n",
    "            recommendations=recommendations\n",
    "        ))\n",
    "    \n",
    "    def _analyze_real_time_integration(self):\n",
    "        \"\"\"Analyze real-time data integration effectiveness\"\"\"\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "        score = 60.0  # Assume needs improvement\n",
    "        \n",
    "        # Check for API services\n",
    "        services_path = self.src_path / \"services\"\n",
    "        if services_path.exists():\n",
    "            api_files = list(services_path.glob(\"*api*.ts*\"))\n",
    "            if api_files:\n",
    "                score += 20\n",
    "                # Check for real-time patterns\n",
    "                for api_file in api_files:\n",
    "                    content = api_file.read_text()\n",
    "                    if \"WebSocket\" in content or \"socket.io\" in content:\n",
    "                        score += 15\n",
    "                    if \"EventSource\" in content or \"SSE\" in content:\n",
    "                        score += 10\n",
    "                    if \"mock\" in content.lower() or \"fake\" in content.lower():\n",
    "                        issues.append(f\"Mock data detected in {api_file.name}\")\n",
    "                        score -= 10\n",
    "        else:\n",
    "            issues.append(\"API services directory not found\")\n",
    "            score -= 20\n",
    "            \n",
    "        # Check for WebSocket context\n",
    "        ws_context = self.src_path / \"context\" / \"WebSocketContext.tsx\"\n",
    "        if ws_context.exists():\n",
    "            score += 15\n",
    "        else:\n",
    "            issues.append(\"WebSocket context not found\")\n",
    "            score -= 10\n",
    "            \n",
    "        status = self._get_status_from_score(score)\n",
    "        \n",
    "        if score < 90:\n",
    "            recommendations.append(\"Replace all mock data with real-time API calls\")\n",
    "        if score < 70:\n",
    "            recommendations.append(\"Implement comprehensive WebSocket integration\")\n",
    "            \n",
    "        self.metrics.append(UIEffectivenessMetric(\n",
    "            name=\"Real-time Integration\",\n",
    "            current_score=score,\n",
    "            target_score=100.0,\n",
    "            status=status,\n",
    "            issues=issues,\n",
    "            recommendations=recommendations\n",
    "        ))\n",
    "    \n",
    "    def _analyze_error_handling(self):\n",
    "        \"\"\"Analyze error handling effectiveness\"\"\"\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "        score = 75.0\n",
    "        \n",
    "        # Check for Error Boundary\n",
    "        error_boundary_files = list(self.src_path.glob(\"**/ErrorBoundary.*\"))\n",
    "        if error_boundary_files:\n",
    "            score += 15\n",
    "        else:\n",
    "            issues.append(\"No Error Boundary component found\")\n",
    "            score -= 15\n",
    "            \n",
    "        # Check for try-catch in components\n",
    "        tsx_files = list(self.src_path.glob(\"**/*.tsx\"))\n",
    "        try_catch_count = 0\n",
    "        for tsx_file in tsx_files:\n",
    "            content = tsx_file.read_text()\n",
    "            try_catch_count += len(re.findall(r'try\\s*{', content))\n",
    "            \n",
    "        if try_catch_count > 5:\n",
    "            score += 10\n",
    "        elif try_catch_count == 0:\n",
    "            issues.append(\"No try-catch error handling found\")\n",
    "            score -= 10\n",
    "            \n",
    "        status = self._get_status_from_score(score)\n",
    "        \n",
    "        if score < 90:\n",
    "            recommendations.append(\"Add comprehensive error boundaries\")\n",
    "            recommendations.append(\"Implement graceful error states in all components\")\n",
    "            \n",
    "        self.metrics.append(UIEffectivenessMetric(\n",
    "            name=\"Error Handling\",\n",
    "            current_score=score,\n",
    "            target_score=100.0,\n",
    "            status=status,\n",
    "            issues=issues,\n",
    "            recommendations=recommendations\n",
    "        ))\n",
    "    \n",
    "    def _analyze_responsive_design(self):\n",
    "        \"\"\"Analyze responsive design effectiveness\"\"\"\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "        score = 70.0\n",
    "        \n",
    "        # Check for Tailwind config\n",
    "        tailwind_config = self.frontend_path / \"tailwind.config.js\"\n",
    "        if tailwind_config.exists():\n",
    "            score += 15\n",
    "            content = tailwind_config.read_text()\n",
    "            if \"responsive\" in content or \"screens\" in content:\n",
    "                score += 10\n",
    "        else:\n",
    "            issues.append(\"Tailwind config not found\")\n",
    "            score -= 10\n",
    "            \n",
    "        # Check for responsive classes in components\n",
    "        tsx_files = list(self.src_path.glob(\"**/*.tsx\"))\n",
    "        responsive_count = 0\n",
    "        for tsx_file in tsx_files:\n",
    "            content = tsx_file.read_text()\n",
    "            responsive_count += len(re.findall(r'(sm:|md:|lg:|xl:)', content))\n",
    "            \n",
    "        if responsive_count > 50:\n",
    "            score += 15\n",
    "        elif responsive_count < 10:\n",
    "            issues.append(\"Limited responsive design classes found\")\n",
    "            score -= 15\n",
    "            \n",
    "        status = self._get_status_from_score(score)\n",
    "        \n",
    "        if score < 90:\n",
    "            recommendations.append(\"Implement mobile-first responsive design\")\n",
    "            recommendations.append(\"Add comprehensive breakpoint handling\")\n",
    "            \n",
    "        self.metrics.append(UIEffectivenessMetric(\n",
    "            name=\"Responsive Design\",\n",
    "            current_score=score,\n",
    "            target_score=100.0,\n",
    "            status=status,\n",
    "            issues=issues,\n",
    "            recommendations=recommendations\n",
    "        ))\n",
    "    \n",
    "    def _analyze_accessibility(self):\n",
    "        \"\"\"Analyze accessibility effectiveness\"\"\"\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "        score = 65.0  # Assume needs improvement\n",
    "        \n",
    "        # Check for ARIA attributes\n",
    "        tsx_files = list(self.src_path.glob(\"**/*.tsx\"))\n",
    "        aria_count = 0\n",
    "        alt_count = 0\n",
    "        \n",
    "        for tsx_file in tsx_files:\n",
    "            content = tsx_file.read_text()\n",
    "            aria_count += len(re.findall(r'aria-\\w+', content))\n",
    "            alt_count += len(re.findall(r'alt=', content))\n",
    "            \n",
    "        if aria_count > 20:\n",
    "            score += 20\n",
    "        elif aria_count < 5:\n",
    "            issues.append(\"Limited ARIA attributes found\")\n",
    "            score -= 10\n",
    "            \n",
    "        if alt_count > 5:\n",
    "            score += 10\n",
    "        else:\n",
    "            issues.append(\"Missing alt attributes for images\")\n",
    "            score -= 10\n",
    "            \n",
    "        # Check for semantic HTML\n",
    "        semantic_elements = ['main', 'nav', 'header', 'footer', 'section', 'article']\n",
    "        semantic_count = 0\n",
    "        for tsx_file in tsx_files:\n",
    "            content = tsx_file.read_text()\n",
    "            for element in semantic_elements:\n",
    "                semantic_count += len(re.findall(f'<{element}[\\\\s>]', content))\n",
    "                \n",
    "        if semantic_count > 10:\n",
    "            score += 15\n",
    "        else:\n",
    "            issues.append(\"Limited semantic HTML elements\")\n",
    "            score -= 5\n",
    "            \n",
    "        status = self._get_status_from_score(score)\n",
    "        \n",
    "        if score < 90:\n",
    "            recommendations.append(\"Implement WCAG 2.1 AA compliance\")\n",
    "            recommendations.append(\"Add comprehensive ARIA attributes\")\n",
    "            recommendations.append(\"Use semantic HTML elements\")\n",
    "            \n",
    "        self.metrics.append(UIEffectivenessMetric(\n",
    "            name=\"Accessibility\",\n",
    "            current_score=score,\n",
    "            target_score=100.0,\n",
    "            status=status,\n",
    "            issues=issues,\n",
    "            recommendations=recommendations\n",
    "        ))\n",
    "    \n",
    "    def _analyze_performance(self):\n",
    "        \"\"\"Analyze performance effectiveness\"\"\"\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "        score = 80.0  # Good baseline with React 18\n",
    "        \n",
    "        # Check for performance optimizations\n",
    "        tsx_files = list(self.src_path.glob(\"**/*.tsx\"))\n",
    "        memo_count = 0\n",
    "        callback_count = 0\n",
    "        lazy_count = 0\n",
    "        \n",
    "        for tsx_file in tsx_files:\n",
    "            content = tsx_file.read_text()\n",
    "            memo_count += len(re.findall(r'React\\.memo|useMemo', content))\n",
    "            callback_count += len(re.findall(r'useCallback', content))\n",
    "            lazy_count += len(re.findall(r'React\\.lazy|lazy\\(', content))\n",
    "            \n",
    "        if memo_count > 5:\n",
    "            score += 5\n",
    "        if callback_count > 3:\n",
    "            score += 5\n",
    "        if lazy_count > 0:\n",
    "            score += 10\n",
    "        else:\n",
    "            issues.append(\"No lazy loading implemented\")\n",
    "            score -= 10\n",
    "            \n",
    "        # Check Vite config for optimizations\n",
    "        vite_config = self.frontend_path / \"vite.config.ts\"\n",
    "        if vite_config.exists():\n",
    "            content = vite_config.read_text()\n",
    "            if \"build\" in content and \"rollupOptions\" in content:\n",
    "                score += 5\n",
    "                \n",
    "        status = self._get_status_from_score(score)\n",
    "        \n",
    "        if score < 90:\n",
    "            recommendations.append(\"Implement code splitting with React.lazy\")\n",
    "            recommendations.append(\"Add performance monitoring\")\n",
    "            \n",
    "        self.metrics.append(UIEffectivenessMetric(\n",
    "            name=\"Performance\",\n",
    "            current_score=score,\n",
    "            target_score=100.0,\n",
    "            status=status,\n",
    "            issues=issues,\n",
    "            recommendations=recommendations\n",
    "        ))\n",
    "    \n",
    "    def _analyze_agent_integration(self):\n",
    "        \"\"\"Analyze agent integration effectiveness\"\"\"\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "        score = 40.0  # Likely needs significant work\n",
    "        \n",
    "        # Check for agent-related files\n",
    "        agent_files = []\n",
    "        for pattern in [\"*agent*\", \"*Agent*\"]:\n",
    "            agent_files.extend(list(self.src_path.glob(f\"**/{pattern}.tsx\")))\n",
    "            \n",
    "        if agent_files:\n",
    "            score += 30\n",
    "            # Check for sophisticated agent integration\n",
    "            for agent_file in agent_files:\n",
    "                content = agent_file.read_text()\n",
    "                if \"sophisticated\" in content or \"advanced\" in content:\n",
    "                    score += 10\n",
    "        else:\n",
    "            issues.append(\"No agent integration components found\")\n",
    "            \n",
    "        # Check for WebSocket agent communication\n",
    "        if len([f for f in agent_files if \"websocket\" in f.read_text().lower() or \"socket\" in f.read_text().lower()]) > 0:\n",
    "            score += 20\n",
    "        else:\n",
    "            issues.append(\"No WebSocket integration for agents\")\n",
    "            score -= 10\n",
    "            \n",
    "        status = self._get_status_from_score(score)\n",
    "        \n",
    "        if score < 90:\n",
    "            recommendations.append(\"Integrate 15 sophisticated agents into UI\")\n",
    "            recommendations.append(\"Add real-time agent communication\")\n",
    "            recommendations.append(\"Create agent management interfaces\")\n",
    "            \n",
    "        self.metrics.append(UIEffectivenessMetric(\n",
    "            name=\"Agent Integration\",\n",
    "            current_score=score,\n",
    "            target_score=100.0,\n",
    "            status=status,\n",
    "            issues=issues,\n",
    "            recommendations=recommendations\n",
    "        ))\n",
    "    \n",
    "    def _analyze_theme_consistency(self):\n",
    "        \"\"\"Analyze theme consistency effectiveness\"\"\"\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "        score = 85.0  # Generally good with modern systems\n",
    "        \n",
    "        # Check for theme context\n",
    "        theme_files = list(self.src_path.glob(\"**/Theme*.tsx\")) + list(self.src_path.glob(\"**/theme*.ts\"))\n",
    "        if theme_files:\n",
    "            score += 10\n",
    "        else:\n",
    "            issues.append(\"No theme system files found\")\n",
    "            score -= 15\n",
    "            \n",
    "        # Check CSS files for consistency\n",
    "        css_files = list(self.src_path.glob(\"**/*.css\"))\n",
    "        if len(css_files) > 0:\n",
    "            score += 5\n",
    "            \n",
    "        status = self._get_status_from_score(score)\n",
    "        \n",
    "        if score < 90:\n",
    "            recommendations.append(\"Ensure consistent theming across all components\")\n",
    "            \n",
    "        self.metrics.append(UIEffectivenessMetric(\n",
    "            name=\"Theme Consistency\",\n",
    "            current_score=score,\n",
    "            target_score=100.0,\n",
    "            status=status,\n",
    "            issues=issues,\n",
    "            recommendations=recommendations\n",
    "        ))\n",
    "    \n",
    "    def _analyze_websocket_integration(self):\n",
    "        \"\"\"Analyze WebSocket integration effectiveness\"\"\"\n",
    "        issues = []\n",
    "        recommendations = []\n",
    "        score = 70.0\n",
    "        \n",
    "        # Check for WebSocket context and implementation\n",
    "        ws_files = list(self.src_path.glob(\"**/WebSocket*.tsx\")) + list(self.src_path.glob(\"**/websocket*.ts\"))\n",
    "        if ws_files:\n",
    "            score += 20\n",
    "            for ws_file in ws_files:\n",
    "                content = ws_file.read_text()\n",
    "                if \"reconnect\" in content or \"retry\" in content:\n",
    "                    score += 10\n",
    "                if \"heartbeat\" in content or \"ping\" in content:\n",
    "                    score += 5\n",
    "        else:\n",
    "            issues.append(\"WebSocket integration files not found\")\n",
    "            score -= 20\n",
    "            \n",
    "        status = self._get_status_from_score(score)\n",
    "        \n",
    "        if score < 90:\n",
    "            recommendations.append(\"Implement robust WebSocket connection management\")\n",
    "            recommendations.append(\"Add auto-reconnection and error handling\")\n",
    "            \n",
    "        self.metrics.append(UIEffectivenessMetric(\n",
    "            name=\"WebSocket Integration\",\n",
    "            current_score=score,\n",
    "            target_score=100.0,\n",
    "            status=status,\n",
    "            issues=issues,\n",
    "            recommendations=recommendations\n",
    "        ))\n",
    "    \n",
    "    def _get_status_from_score(self, score: float) -> str:\n",
    "        \"\"\"Get status from numerical score\"\"\"\n",
    "        if score >= 90:\n",
    "            return \"excellent\"\n",
    "        elif score >= 75:\n",
    "            return \"good\"\n",
    "        elif score >= 60:\n",
    "            return \"needs_improvement\"\n",
    "        else:\n",
    "            return \"critical\"\n",
    "    \n",
    "    def _generate_prioritized_recommendations(self) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Generate prioritized recommendations for improvement\"\"\"\n",
    "        all_recommendations = []\n",
    "        \n",
    "        for metric in self.metrics:\n",
    "            for rec in metric.recommendations:\n",
    "                priority = \"high\" if metric.current_score < 60 else \"medium\" if metric.current_score < 80 else \"low\"\n",
    "                all_recommendations.append({\n",
    "                    \"metric\": metric.name,\n",
    "                    \"recommendation\": rec,\n",
    "                    \"priority\": priority,\n",
    "                    \"current_score\": metric.current_score,\n",
    "                    \"impact\": \"high\" if metric.current_score < 70 else \"medium\"\n",
    "                })\n",
    "        \n",
    "        # Sort by priority and impact\n",
    "        priority_order = {\"high\": 3, \"medium\": 2, \"low\": 1}\n",
    "        impact_order = {\"high\": 3, \"medium\": 2, \"low\": 1}\n",
    "        \n",
    "        return sorted(all_recommendations, \n",
    "                     key=lambda x: (priority_order.get(x[\"priority\"], 0), \n",
    "                                   impact_order.get(x[\"impact\"], 0)), \n",
    "                     reverse=True)\n",
    "    \n",
    "    def _estimate_implementation_timeline(self) -> Dict[str, Any]:\n",
    "        \"\"\"Estimate implementation timeline\"\"\"\n",
    "        critical_metrics = len([m for m in self.metrics if m.status == \"critical\"])\n",
    "        improvement_metrics = len([m for m in self.metrics if m.status == \"needs_improvement\"])\n",
    "        \n",
    "        days_estimate = critical_metrics * 3 + improvement_metrics * 2\n",
    "        \n",
    "        return {\n",
    "            \"estimated_days\": days_estimate,\n",
    "            \"estimated_weeks\": days_estimate / 5,\n",
    "            \"critical_issues\": critical_metrics,\n",
    "            \"improvement_issues\": improvement_metrics,\n",
    "            \"phases\": [\n",
    "                {\"phase\": \"Critical Fixes\", \"duration_days\": critical_metrics * 3},\n",
    "                {\"phase\": \"Improvements\", \"duration_days\": improvement_metrics * 2},\n",
    "                {\"phase\": \"Testing & Polish\", \"duration_days\": 3}\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def generate_report(self) -> str:\n",
    "        \"\"\"Generate comprehensive effectiveness report\"\"\"\n",
    "        analysis = self.analyze_all_metrics()\n",
    "        \n",
    "        report = f\"\"\"\n",
    "üéØ CARTRITA UI/UX EFFECTIVENESS REPORT\n",
    "{'=' * 50}\n",
    "\n",
    "üìä OVERALL EFFECTIVENESS: {analysis['overall_effectiveness']:.1f}/100\n",
    "üéØ TARGET EFFECTIVENESS: {analysis['target_effectiveness']:.0f}/100\n",
    "üìâ EFFECTIVENESS GAP: {analysis['effectiveness_gap']:.1f} points\n",
    "\n",
    "üìà METRICS BREAKDOWN:\n",
    "\"\"\"\n",
    "        \n",
    "        for metric in analysis['metrics']:\n",
    "            status_emoji = {\n",
    "                'excellent': 'üü¢',\n",
    "                'good': 'üü°', \n",
    "                'needs_improvement': 'üü†',\n",
    "                'critical': 'üî¥'\n",
    "            }\n",
    "            \n",
    "            report += f\"\"\"\n",
    "{status_emoji.get(metric['status'], '‚ö™')} {metric['name']}: {metric['current_score']:.1f}/100 ({metric['status'].upper()})\n",
    "   Issues: {len(metric['issues'])} | Recommendations: {len(metric['recommendations'])}\n",
    "\"\"\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "üö® CRITICAL ISSUES: {len(analysis['critical_issues'])}\n",
    "\"\"\"\n",
    "        \n",
    "        for critical in analysis['critical_issues']:\n",
    "            report += f\"   ‚Ä¢ {critical['name']}: {critical['current_score']:.1f}/100\\n\"\n",
    "        \n",
    "        report += f\"\"\"\n",
    "üîß TOP PRIORITY RECOMMENDATIONS:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, rec in enumerate(analysis['recommendations'][:5], 1):\n",
    "            priority_emoji = {'high': 'üî•', 'medium': '‚ö°', 'low': 'üí°'}\n",
    "            report += f\"   {i}. {priority_emoji.get(rec['priority'], '‚Ä¢')} {rec['recommendation']} ({rec['metric']})\\n\"\n",
    "        \n",
    "        timeline = analysis['implementation_timeline']\n",
    "        report += f\"\"\"\n",
    "‚è±Ô∏è IMPLEMENTATION TIMELINE:\n",
    "   Total Estimated Time: {timeline['estimated_days']} days ({timeline['estimated_weeks']:.1f} weeks)\n",
    "   Critical Issues: {timeline['critical_issues']} ({timeline['critical_issues'] * 3} days)\n",
    "   Improvements: {timeline['improvement_issues']} ({timeline['improvement_issues'] * 2} days)\n",
    "\n",
    "üéØ NEXT STEPS:\n",
    "   1. Address critical UI/UX issues immediately\n",
    "   2. Implement real-time agent integration\n",
    "   3. Replace mock data with live backend calls\n",
    "   4. Optimize mobile responsiveness\n",
    "   5. Add comprehensive error handling\n",
    "   \n",
    "üìã EFFECTIVENESS STATUS: {'EXCELLENT' if analysis['overall_effectiveness'] >= 90 else 'GOOD' if analysis['overall_effectiveness'] >= 75 else 'NEEDS IMPROVEMENT' if analysis['overall_effectiveness'] >= 60 else 'CRITICAL'}\n",
    "\"\"\"\n",
    "        \n",
    "        return report\n",
    "\n",
    "# Execute the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"üöÄ Starting Cartrita UI/UX Effectiveness Analysis...\")\n",
    "    analyzer = CartritaUIAnalyzer()\n",
    "    \n",
    "    # Generate and display report\n",
    "    report = analyzer.generate_report()\n",
    "    print(report)\n",
    "    \n",
    "    # Save detailed analysis\n",
    "    analysis = analyzer.analyze_all_metrics()\n",
    "    with open(\"/tmp/cartrita_ui_effectiveness_analysis.json\", \"w\") as f:\n",
    "        json.dump(analysis, f, indent=2, default=str)\n",
    "    \n",
    "    print(f\"\\nüíæ Detailed analysis saved to: /tmp/cartrita_ui_effectiveness_analysis.json\")\n",
    "    print(f\"‚è±Ô∏è Analysis completed in: {analysis['analysis_duration']}\")\n",
    "    print(\"\\nüéØ Ready to implement 100% UI/UX effectiveness!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8517d85e",
   "metadata": {},
   "source": [
    "# Cartrita V2 Design & Implementation Notebook\n",
    "\n",
    "## **The Revolutionary Hybrid Multi-Agent MCP System**\n",
    "### **Complete Migration Guide from V1 to V2 Architecture**\n",
    "\n",
    "---\n",
    "\n",
    "**DBC**: Data-driven AI Tool that Applies Behavioral Intelligence Tools while Connecting Humanity.  \n",
    "**Cartrita**: Cognitive AI Reasoning Tool for Real-time Information and Task Automation.\n",
    "\n",
    "This notebook represents the complete design specification and implementation guide for migrating Cartrita from its current V1 architecture (Iterations 18-22) to the revolutionary V2 system featuring:\n",
    "\n",
    "- **Hybrid Node.js/Python Stack** with clear separation of concerns\n",
    "- **LangChain/LangGraph StateGraph Orchestration** for deterministic agent workflows\n",
    "- **MCP (Model Context Protocol)** for standardized tool integration\n",
    "- **Adaptive RAG Pipeline** with budget-aware context management\n",
    "- **Hierarchical Planner/Decomposer** with event-driven reactive loops\n",
    "- **Production-grade Observability** with OpenTelemetry integration\n",
    "\n",
    "---\n",
    "\n",
    "## V2 Strategic Objectives\n",
    "\n",
    "**Goal**: Transform Cartrita from an organic V1 system into a transparent, event-driven, graph-orchestrated architecture that enables safer scaling, easier experimentation, and better cost control without sacrificing personality or capability breadth.\n",
    "\n",
    "### Key V2 Improvements Over V1:\n",
    "1. **Deterministic Control Flow** - LangGraph StateGraph replaces ad-hoc orchestration\n",
    "2. **Unified Tool Registry** - MCP protocol standardizes Node.js and Python tools\n",
    "3. **Event-Driven Architecture** - NATS message bus enables fine-grained reactive loops\n",
    "4. **Adaptive Resource Management** - Budget-aware context shaping and model selection\n",
    "5. **Hierarchical Planning** - Traceable task decomposition with policy enforcement\n",
    "6. **Enhanced Observability** - Complete trace visibility across agent operations\n",
    "\n",
    "### V1 ‚Üí V2 Migration Benefits:\n",
    "- **‚â•10% Cost Reduction** through intelligent resource management\n",
    "- **Improved Plan Accuracy** via structured decomposition and critic feedback\n",
    "- **Better Developer Experience** - Add new agents in <1 hour\n",
    "- **Production Observability** - Trace any decision path with single query\n",
    "- **Fault Tolerance** - Graceful degradation and automatic recovery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1085e7",
   "metadata": {},
   "source": [
    "# 1. Environment Setup and Dependencies\n",
    "\n",
    "This section covers the complete setup of the V2 development environment, including all required dependencies, runtime versions, and monorepo structure initialization.\n",
    "\n",
    "## 1.1 System Requirements\n",
    "\n",
    "### Core Runtime Requirements\n",
    "- **Node.js**: 22+ with pnpm package manager\n",
    "- **Python**: 3.11+ with uv or Poetry\n",
    "- **PostgreSQL**: 14+ with pgvector extension\n",
    "- **NATS**: Latest for event streaming\n",
    "- **Redis**: 7+ for caching and rate limiting\n",
    "- **Docker**: For containerized development\n",
    "\n",
    "### Hardware Recommendations\n",
    "- **Memory**: 16GB+ RAM for local development\n",
    "- **Storage**: 50GB+ available space\n",
    "- **CPU**: Multi-core processor (4+ cores recommended)\n",
    "- **Network**: Stable internet for AI API calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "526f257d",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1513388195.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    set -euo pipefail\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# V2 Environment Setup Script\n",
    "set -euo pipefail\n",
    "\n",
    "echo \"üöÄ Setting up Cartrita V2 Development Environment...\"\n",
    "\n",
    "# 1. Verify system requirements\n",
    "echo \"üìã Checking system requirements...\"\n",
    "node_version=$(node --version | cut -d'v' -f2 | cut -d'.' -f1)\n",
    "if [ \"$node_version\" -lt 22 ]; then\n",
    "    echo \"‚ùå Node.js 22+ required. Current: $(node --version)\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "python_version=$(python3 --version | cut -d' ' -f2 | cut -d'.' -f1,2)\n",
    "if ! python3 -c \"import sys; sys.exit(0 if sys.version_info >= (3,11) else 1)\"; then\n",
    "    echo \"‚ùå Python 3.11+ required. Current: $python_version\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "# 2. Install package managers\n",
    "echo \"üì¶ Installing package managers...\"\n",
    "npm install -g pnpm\n",
    "curl -LsSf https://astral.sh/uv/install.sh | sh\n",
    "source $HOME/.cargo/env\n",
    "\n",
    "# 3. Initialize monorepo structure\n",
    "echo \"üèóÔ∏è  Creating V2 monorepo structure...\"\n",
    "mkdir -p {gateway-node,orchestrator-python,agents,tools-node,tools-python,memory,governance,shared/{protos,schemas,prompts},infra/{terraform,k8s,docker},test/{simulation,load,contract}}\n",
    "\n",
    "# 4. Setup Node.js workspace\n",
    "echo \"üîß Setting up Node.js workspace...\"\n",
    "cat > package.json << EOF\n",
    "{\n",
    "  \"name\": \"cartrita-v2\",\n",
    "  \"version\": \"2.0.0\",\n",
    "  \"description\": \"Revolutionary Hybrid Multi-Agent MCP System\",\n",
    "  \"workspaces\": [\n",
    "    \"gateway-node\",\n",
    "    \"tools-node/*\",\n",
    "    \"shared/schemas\"\n",
    "  ],\n",
    "  \"scripts\": {\n",
    "    \"dev\": \"concurrently \\\"pnpm dev:gateway\\\" \\\"pnpm dev:orchestrator\\\"\",\n",
    "    \"dev:gateway\": \"pnpm --filter gateway-node dev\",\n",
    "    \"dev:orchestrator\": \"cd orchestrator-python && uv run main.py --dev\",\n",
    "    \"build\": \"pnpm --recursive build\",\n",
    "    \"test\": \"pnpm --recursive test\",\n",
    "    \"migrate\": \"make migrate\",\n",
    "    \"deploy\": \"make deploy\"\n",
    "  },\n",
    "  \"devDependencies\": {\n",
    "    \"concurrently\": \"^8.2.2\",\n",
    "    \"typescript\": \"^5.3.0\",\n",
    "    \"@types/node\": \"^20.10.0\"\n",
    "  }\n",
    "}\n",
    "EOF\n",
    "\n",
    "pnpm install\n",
    "\n",
    "echo \"‚úÖ V2 environment setup complete!\"\n",
    "echo \"üî• Ready to build the future of AI orchestration!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d655cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üêç Setting up Python V2 environment...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[33mwarning\u001b[39m\u001b[0m\u001b[1m:\u001b[0m \u001b[1m`VIRTUAL_ENV=/home/robbie/development/dat-bitch-cartrita/.venv` does not match the project environment path `.venv` and will be ignored; use `--active` to target the active environment instead\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m cpython-3.13.6-linux-x86_64-gnu (download) \u001b[2m(30.8MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m cpython-3.13.6-linux-x86_64-gnu (download) \u001b[2m(30.8MiB)\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m cpython-3.13.6-linux-x86_64-gnu (download)\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m cpython-3.13.6-linux-x86_64-gnu (download)\n",
      "Using CPython \u001b[36m3.13.6\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "Using CPython \u001b[36m3.13.6\u001b[39m\n",
      "Creating virtual environment at: \u001b[36m.venv\u001b[39m\n",
      "\u001b[2mResolved \u001b[1m79 packages\u001b[0m \u001b[2min 1.74s\u001b[0m\u001b[0m\n",
      "   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m cartrita-v2-orchestrator\u001b[2m @ file:///home/robbie/development/dat-bitch-cartrita/docs/orchestrator-python\u001b[0m\n",
      "\u001b[2mResolved \u001b[1m79 packages\u001b[0m \u001b[2min 1.74s\u001b[0m\u001b[0m\n",
      "   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m cartrita-v2-orchestrator\u001b[2m @ file:///home/robbie/development/dat-bitch-cartrita/docs/orchestrator-python\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m ruff \u001b[2m(12.4MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m mypy \u001b[2m(12.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m psycopg-binary \u001b[2m(4.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m black \u001b[2m(1.7MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m zstandard \u001b[2m(5.3MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pydantic-core \u001b[2m(1.9MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m numpy \u001b[2m(15.9MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m tiktoken \u001b[2m(1.1MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pygments \u001b[2m(1.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m sqlalchemy \u001b[2m(3.1MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m ruff \u001b[2m(12.4MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m mypy \u001b[2m(12.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m psycopg-binary \u001b[2m(4.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m black \u001b[2m(1.7MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m zstandard \u001b[2m(5.3MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pydantic-core \u001b[2m(1.9MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m numpy \u001b[2m(15.9MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m tiktoken \u001b[2m(1.1MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m pygments \u001b[2m(1.2MiB)\u001b[0m\n",
      "\u001b[36m\u001b[1mDownloading\u001b[0m\u001b[39m sqlalchemy \u001b[2m(3.1MiB)\u001b[0m\n",
      "   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m nats-py\u001b[2m==2.11.0\u001b[0m\n",
      "   \u001b[36m\u001b[1mBuilding\u001b[0m\u001b[39m nats-py\u001b[2m==2.11.0\u001b[0m\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m tiktoken\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pygments\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m tiktoken\n",
      " \u001b[32m\u001b[1mDownloading\u001b[0m\u001b[39m pygments\n",
      "  \u001b[31m√ó\u001b[0m Failed to build `cartrita-v2-orchestrator @\n",
      "  \u001b[31m‚îÇ\u001b[0m file:///home/robbie/development/dat-bitch-cartrita/docs/orchestrator-python`\n",
      "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n",
      "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `hatchling.build.build_editable` failed (exit status: 1)\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
      "\u001b[31m      \u001b[0mTraceback (most recent call last):\n",
      "\u001b[31m      \u001b[0m  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m11\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "\u001b[31m      \u001b[0m    wheel_filename =\n",
      "\u001b[31m      \u001b[0mbackend.build_editable(\"/home/robbie/.cache/uv/builds-v0/.tmp6jlwFx\",\n",
      "\u001b[31m      \u001b[0m{}, None)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/build.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m83\u001b[0m, in \u001b[35mbuild_editable\u001b[0m\n",
      "\u001b[31m      \u001b[0m    return os.path.basename(\u001b[31mnext\u001b[0m\u001b[1;31m(builder.build(directory=wheel_directory,\n",
      "\u001b[31m      \u001b[0mversions=['editable']))\u001b[0m)\n",
      "\u001b[31m      \u001b[0m\n",
      "\u001b[31m      \u001b[0m\u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/plugin/interface.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m155\u001b[0m, in \u001b[35mbuild\u001b[0m\n",
      "\u001b[31m      \u001b[0m    artifact = version_api[version](directory, **build_data)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/wheel.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m496\u001b[0m, in \u001b[35mbuild_editable\u001b[0m\n",
      "\u001b[31m      \u001b[0m    return \u001b[31mself.build_editable_detection\u001b[0m\u001b[1;31m(directory, **build_data)\u001b[0m\n",
      "\u001b[31m      \u001b[0m           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/wheel.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m507\u001b[0m, in \u001b[35mbuild_editable_detection\u001b[0m\n",
      "\u001b[31m      \u001b[0m    for included_file in \u001b[31mself.recurse_selected_project_files\u001b[0m\u001b[1;31m()\u001b[0m:\n",
      "\u001b[31m      \u001b[0m                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/plugin/interface.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m180\u001b[0m, in \u001b[35mrecurse_selected_project_files\u001b[0m\n",
      "\u001b[31m      \u001b[0m    if \u001b[1;31mself.config.only_include\u001b[0m:\n",
      "\u001b[31m      \u001b[0m       \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.local/share/uv/python/cpython-3.13.6-linux-x86_64-gnu/lib/python3.13/functools.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m1026\u001b[0m, in \u001b[35m__get__\u001b[0m\n",
      "\u001b[31m      \u001b[0m    val = self.func(instance)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/config.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m713\u001b[0m, in \u001b[35monly_include\u001b[0m\n",
      "\u001b[31m      \u001b[0m    only_include = only_include_config.get('only-include',\n",
      "\u001b[31m      \u001b[0m\u001b[31mself.default_only_include\u001b[0m\u001b[1;31m()\u001b[0m) or self.packages\n",
      "\u001b[31m      \u001b[0m\n",
      "\u001b[31m      \u001b[0m\u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/wheel.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m262\u001b[0m, in \u001b[35mdefault_only_include\u001b[0m\n",
      "\u001b[31m      \u001b[0m    return \u001b[1;31mself.default_file_selection_options\u001b[0m.only_include\n",
      "\u001b[31m      \u001b[0m           \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.local/share/uv/python/cpython-3.13.6-linux-x86_64-gnu/lib/python3.13/functools.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m1026\u001b[0m, in \u001b[35m__get__\u001b[0m\n",
      "\u001b[31m      \u001b[0m    val = self.func(instance)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/wheel.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m250\u001b[0m, in \u001b[35mdefault_file_selection_options\u001b[0m\n",
      "\u001b[31m      \u001b[0m    raise ValueError(message)\n",
      "\u001b[31m      \u001b[0m\u001b[1;35mValueError\u001b[0m: \u001b[35mUnable to determine which files to ship\n",
      "\u001b[31m      \u001b[0minside the wheel using the following heuristics:\n",
      "\u001b[31m      \u001b[0mhttps://hatch.pypa.io/latest/plugins/builder/wheel/#default-file-selection\n",
      "\n",
      "\u001b[31m      \u001b[0mThe most likely cause of this is that there is no directory that matches\n",
      "\u001b[31m      \u001b[0mthe name of your project (cartrita_v2_orchestrator).\n",
      "\n",
      "\u001b[31m      \u001b[0mAt least one file selection option must be defined\n",
      "\u001b[31m      \u001b[0min the `tool.hatch.build.targets.wheel` table, see:\n",
      "\u001b[31m      \u001b[0mhttps://hatch.pypa.io/latest/config/build/\n",
      "\n",
      "\u001b[31m      \u001b[0mAs an example, if you intend to ship a directory named `foo` that\n",
      "\u001b[31m      \u001b[0mresides within a `src` directory located at the root of your project,\n",
      "\u001b[31m      \u001b[0myou can define the following:\n",
      "\n",
      "\u001b[31m      \u001b[0m[tool.hatch.build.targets.wheel]\n",
      "\u001b[31m      \u001b[0mpackages = [\"src/foo\"]\u001b[0m\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
      "\u001b[31m      \u001b[0menvironment.\n",
      "  \u001b[31m√ó\u001b[0m Failed to build `cartrita-v2-orchestrator @\n",
      "  \u001b[31m‚îÇ\u001b[0m file:///home/robbie/development/dat-bitch-cartrita/docs/orchestrator-python`\n",
      "\u001b[31m  ‚îú‚îÄ‚ñ∂ \u001b[0mThe build backend returned an error\n",
      "\u001b[31m  ‚ï∞‚îÄ‚ñ∂ \u001b[0mCall to `hatchling.build.build_editable` failed (exit status: 1)\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[31m[stderr]\u001b[39m\n",
      "\u001b[31m      \u001b[0mTraceback (most recent call last):\n",
      "\u001b[31m      \u001b[0m  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m11\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "\u001b[31m      \u001b[0m    wheel_filename =\n",
      "\u001b[31m      \u001b[0mbackend.build_editable(\"/home/robbie/.cache/uv/builds-v0/.tmp6jlwFx\",\n",
      "\u001b[31m      \u001b[0m{}, None)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/build.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m83\u001b[0m, in \u001b[35mbuild_editable\u001b[0m\n",
      "\u001b[31m      \u001b[0m    return os.path.basename(\u001b[31mnext\u001b[0m\u001b[1;31m(builder.build(directory=wheel_directory,\n",
      "\u001b[31m      \u001b[0mversions=['editable']))\u001b[0m)\n",
      "\u001b[31m      \u001b[0m\n",
      "\u001b[31m      \u001b[0m\u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/plugin/interface.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m155\u001b[0m, in \u001b[35mbuild\u001b[0m\n",
      "\u001b[31m      \u001b[0m    artifact = version_api[version](directory, **build_data)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/wheel.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m496\u001b[0m, in \u001b[35mbuild_editable\u001b[0m\n",
      "\u001b[31m      \u001b[0m    return \u001b[31mself.build_editable_detection\u001b[0m\u001b[1;31m(directory, **build_data)\u001b[0m\n",
      "\u001b[31m      \u001b[0m           \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/wheel.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m507\u001b[0m, in \u001b[35mbuild_editable_detection\u001b[0m\n",
      "\u001b[31m      \u001b[0m    for included_file in \u001b[31mself.recurse_selected_project_files\u001b[0m\u001b[1;31m()\u001b[0m:\n",
      "\u001b[31m      \u001b[0m                         \u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/plugin/interface.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m180\u001b[0m, in \u001b[35mrecurse_selected_project_files\u001b[0m\n",
      "\u001b[31m      \u001b[0m    if \u001b[1;31mself.config.only_include\u001b[0m:\n",
      "\u001b[31m      \u001b[0m       \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.local/share/uv/python/cpython-3.13.6-linux-x86_64-gnu/lib/python3.13/functools.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m1026\u001b[0m, in \u001b[35m__get__\u001b[0m\n",
      "\u001b[31m      \u001b[0m    val = self.func(instance)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/config.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m713\u001b[0m, in \u001b[35monly_include\u001b[0m\n",
      "\u001b[31m      \u001b[0m    only_include = only_include_config.get('only-include',\n",
      "\u001b[31m      \u001b[0m\u001b[31mself.default_only_include\u001b[0m\u001b[1;31m()\u001b[0m) or self.packages\n",
      "\u001b[31m      \u001b[0m\n",
      "\u001b[31m      \u001b[0m\u001b[31m~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/wheel.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m262\u001b[0m, in \u001b[35mdefault_only_include\u001b[0m\n",
      "\u001b[31m      \u001b[0m    return \u001b[1;31mself.default_file_selection_options\u001b[0m.only_include\n",
      "\u001b[31m      \u001b[0m           \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.local/share/uv/python/cpython-3.13.6-linux-x86_64-gnu/lib/python3.13/functools.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m1026\u001b[0m, in \u001b[35m__get__\u001b[0m\n",
      "\u001b[31m      \u001b[0m    val = self.func(instance)\n",
      "\u001b[31m      \u001b[0m  File\n",
      "\u001b[31m      \u001b[0m\u001b[35m\"/home/robbie/.cache/uv/builds-v0/.tmpEe9QJz/lib/python3.13/site-packages/hatchling/builders/wheel.py\"\u001b[0m,\n",
      "\u001b[31m      \u001b[0mline \u001b[35m250\u001b[0m, in \u001b[35mdefault_file_selection_options\u001b[0m\n",
      "\u001b[31m      \u001b[0m    raise ValueError(message)\n",
      "\u001b[31m      \u001b[0m\u001b[1;35mValueError\u001b[0m: \u001b[35mUnable to determine which files to ship\n",
      "\u001b[31m      \u001b[0minside the wheel using the following heuristics:\n",
      "\u001b[31m      \u001b[0mhttps://hatch.pypa.io/latest/plugins/builder/wheel/#default-file-selection\n",
      "\n",
      "\u001b[31m      \u001b[0mThe most likely cause of this is that there is no directory that matches\n",
      "\u001b[31m      \u001b[0mthe name of your project (cartrita_v2_orchestrator).\n",
      "\n",
      "\u001b[31m      \u001b[0mAt least one file selection option must be defined\n",
      "\u001b[31m      \u001b[0min the `tool.hatch.build.targets.wheel` table, see:\n",
      "\u001b[31m      \u001b[0mhttps://hatch.pypa.io/latest/config/build/\n",
      "\n",
      "\u001b[31m      \u001b[0mAs an example, if you intend to ship a directory named `foo` that\n",
      "\u001b[31m      \u001b[0mresides within a `src` directory located at the root of your project,\n",
      "\u001b[31m      \u001b[0myou can define the following:\n",
      "\n",
      "\u001b[31m      \u001b[0m[tool.hatch.build.targets.wheel]\n",
      "\u001b[31m      \u001b[0mpackages = [\"src/foo\"]\u001b[0m\n",
      "\n",
      "\u001b[31m      \u001b[0m\u001b[36m\u001b[1mhint\u001b[0m\u001b[39m\u001b[1m:\u001b[0m This usually indicates a problem with the package or the build\n",
      "\u001b[31m      \u001b[0menvironment.\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['uv', 'sync']' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 94\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124müî• LangChain/LangGraph stack ready for orchestration!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 94\u001b[0m     \u001b[43msetup_python_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 83\u001b[0m, in \u001b[0;36msetup_python_environment\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Initialize uv projects\u001b[39;00m\n\u001b[1;32m     82\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124morchestrator-python\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 83\u001b[0m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msync\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     84\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     86\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magents\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/lib/python3.10/subprocess.py:526\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    524\u001b[0m     retcode \u001b[38;5;241m=\u001b[39m process\u001b[38;5;241m.\u001b[39mpoll()\n\u001b[1;32m    525\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[0;32m--> 526\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process\u001b[38;5;241m.\u001b[39margs,\n\u001b[1;32m    527\u001b[0m                                  output\u001b[38;5;241m=\u001b[39mstdout, stderr\u001b[38;5;241m=\u001b[39mstderr)\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process\u001b[38;5;241m.\u001b[39margs, retcode, stdout, stderr)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command '['uv', 'sync']' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "# Python V2 Environment Setup\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def setup_python_environment():\n",
    "    \"\"\"Setup Python environment for Cartrita V2\"\"\"\n",
    "    \n",
    "    print(\"üêç Setting up Python V2 environment...\")\n",
    "    \n",
    "    # Create orchestrator pyproject.toml\n",
    "    orchestrator_config = \"\"\"\n",
    "[project]\n",
    "name = \"cartrita-v2-orchestrator\"\n",
    "version = \"2.0.0\"\n",
    "description = \"Cartrita V2 LangGraph Orchestrator\"\n",
    "requires-python = \">=3.11\"\n",
    "dependencies = [\n",
    "    \"langchain>=0.3.0\",\n",
    "    \"langchain-openai>=0.2.0\",\n",
    "    \"langgraph>=0.2.0\",\n",
    "    \"langsmith>=0.1.0\",\n",
    "    \"fastapi>=0.104.0\",\n",
    "    \"uvicorn>=0.24.0\",\n",
    "    \"pydantic>=2.5.0\",\n",
    "    \"psycopg[binary]>=3.1.0\",\n",
    "    \"redis>=5.0.0\",\n",
    "    \"nats-py>=2.6.0\",\n",
    "    \"opentelemetry-api>=1.21.0\",\n",
    "    \"opentelemetry-sdk>=1.21.0\",\n",
    "    \"opentelemetry-instrumentation-fastapi>=0.42b0\",\n",
    "    \"tenacity>=8.2.0\",\n",
    "    \"tiktoken>=0.5.0\",\n",
    "    \"numpy>=1.24.0\",\n",
    "    \"asyncio-throttle>=1.0.2\"\n",
    "]\n",
    "\n",
    "[build-system]\n",
    "requires = [\"hatchling\"]\n",
    "build-backend = \"hatchling.build\"\n",
    "\n",
    "[tool.uv]\n",
    "dev-dependencies = [\n",
    "    \"pytest>=7.4.0\",\n",
    "    \"pytest-asyncio>=0.21.0\",\n",
    "    \"black>=23.0.0\",\n",
    "    \"isort>=5.12.0\",\n",
    "    \"mypy>=1.7.0\",\n",
    "    \"ruff>=0.1.0\"\n",
    "]\n",
    "\"\"\"\n",
    "    \n",
    "    # Write orchestrator config\n",
    "    orchestrator_dir = Path(\"orchestrator-python\")\n",
    "    orchestrator_dir.mkdir(exist_ok=True)\n",
    "    (orchestrator_dir / \"pyproject.toml\").write_text(orchestrator_config)\n",
    "    \n",
    "    # Create agents pyproject.toml\n",
    "    agents_config = \"\"\"\n",
    "[project]\n",
    "name = \"cartrita-v2-agents\"\n",
    "version = \"2.0.0\"\n",
    "description = \"Cartrita V2 Specialized Agent Microservices\"\n",
    "requires-python = \">=3.11\"\n",
    "dependencies = [\n",
    "    \"langchain>=0.3.0\",\n",
    "    \"langchain-openai>=0.2.0\",\n",
    "    \"pydantic>=2.5.0\",\n",
    "    \"openai>=1.3.0\",\n",
    "    \"httpx>=0.25.0\",\n",
    "    \"asyncio>=3.4.3\",\n",
    "    \"python-multipart>=0.0.6\"\n",
    "]\n",
    "\"\"\"\n",
    "    \n",
    "    agents_dir = Path(\"agents\")\n",
    "    agents_dir.mkdir(exist_ok=True)\n",
    "    (agents_dir / \"pyproject.toml\").write_text(agents_config)\n",
    "    \n",
    "    # Initialize uv projects\n",
    "    os.chdir(\"orchestrator-python\")\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "    os.chdir(\"agents\")\n",
    "    subprocess.run([\"uv\", \"sync\"], check=True)\n",
    "    os.chdir(\"..\")\n",
    "    \n",
    "    print(\"‚úÖ Python V2 environment configured!\")\n",
    "    print(\"üî• LangChain/LangGraph stack ready for orchestration!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    setup_python_environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85990c37",
   "metadata": {},
   "source": [
    "# 2. Database Schema Migration (V1 to V2)\n",
    "\n",
    "This section implements the comprehensive database migration from V1 to V2 architecture, introducing new tables for task hierarchies, graph snapshots, retrieval metrics, and policy decisions while maintaining backward compatibility.\n",
    "\n",
    "## 2.1 V2 Core Schema Design\n",
    "\n",
    "The V2 schema introduces several key concepts:\n",
    "\n",
    "### Core Entity Changes:\n",
    "- **tasks_v2**: Hierarchical task representation with parent/child relationships\n",
    "- **task_edges**: Explicit task dependency graph\n",
    "- **graph_snapshots**: State machine checkpoints for debugging and rollback\n",
    "- **retrieval_metrics**: RAG performance tracking and optimization\n",
    "- **tool_capability_index**: Dynamic tool performance and reliability scoring\n",
    "- **policy_decisions**: Governance and budget enforcement audit trail\n",
    "\n",
    "### Design Principles:\n",
    "1. **Additive Migration**: Never modify existing V1 tables\n",
    "2. **Backward Compatibility**: V1 APIs continue functioning during transition\n",
    "3. **Event Sourcing**: All state changes create immutable event records\n",
    "4. **Observability**: Every operation generates traceable metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856729e6",
   "metadata": {
    "vscode": {
     "languageId": "sql"
    }
   },
   "outputs": [],
   "source": [
    "-- File: db-init/2025_01_20_01_v2_core_migration.sql\n",
    "-- Cartrita V2 Core Schema Migration\n",
    "-- This migration introduces the foundational V2 tables while preserving V1 compatibility\n",
    "\n",
    "BEGIN;\n",
    "\n",
    "-- Enable required extensions\n",
    "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";\n",
    "CREATE EXTENSION IF NOT EXISTS vector;\n",
    "CREATE EXTENSION IF NOT EXISTS pg_trgm;\n",
    "CREATE EXTENSION IF NOT EXISTS btree_gin;\n",
    "\n",
    "-- 1. Hierarchical Task System (V2)\n",
    "CREATE TABLE IF NOT EXISTS tasks_v2 (\n",
    "    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
    "    session_id UUID NOT NULL REFERENCES sessions(id) ON DELETE CASCADE,\n",
    "    parent_id UUID REFERENCES tasks_v2(id) ON DELETE CASCADE,\n",
    "    intent TEXT NOT NULL,\n",
    "    spec_json JSONB NOT NULL DEFAULT '{}',\n",
    "    cost_budget_tokens INTEGER DEFAULT 5000,\n",
    "    priority INTEGER DEFAULT 5 CHECK (priority BETWEEN 1 AND 10),\n",
    "    status VARCHAR(20) DEFAULT 'pending' CHECK (status IN ('pending', 'running', 'complete', 'failed', 'pruned', 'aborted')),\n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n",
    "    updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n",
    "    completed_at TIMESTAMP WITH TIME ZONE,\n",
    "    \n",
    "    -- Metadata\n",
    "    estimated_cost_tokens INTEGER DEFAULT 0,\n",
    "    actual_cost_tokens INTEGER DEFAULT 0,\n",
    "    agent_assigned VARCHAR(50),\n",
    "    error_message TEXT,\n",
    "    retry_count INTEGER DEFAULT 0,\n",
    "    \n",
    "    -- Indexing\n",
    "    CONSTRAINT tasks_v2_parent_not_self CHECK (id != parent_id)\n",
    ");\n",
    "\n",
    "-- Indexes for hierarchical queries\n",
    "CREATE INDEX IF NOT EXISTS idx_tasks_v2_session_status ON tasks_v2(session_id, status);\n",
    "CREATE INDEX IF NOT EXISTS idx_tasks_v2_parent ON tasks_v2(parent_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_tasks_v2_agent_status ON tasks_v2(agent_assigned, status);\n",
    "CREATE INDEX IF NOT EXISTS idx_tasks_v2_priority_status ON tasks_v2(priority DESC, status);\n",
    "\n",
    "-- 2. Task Dependency Graph\n",
    "CREATE TABLE IF NOT EXISTS task_edges (\n",
    "    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
    "    from_task_id UUID NOT NULL REFERENCES tasks_v2(id) ON DELETE CASCADE,\n",
    "    to_task_id UUID NOT NULL REFERENCES tasks_v2(id) ON DELETE CASCADE,\n",
    "    relation_type VARCHAR(20) DEFAULT 'depends_on' CHECK (relation_type IN ('depends_on', 'blocks', 'enables', 'refines')),\n",
    "    weight DECIMAL(3,2) DEFAULT 1.0,\n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n",
    "    \n",
    "    UNIQUE(from_task_id, to_task_id, relation_type)\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_task_edges_from ON task_edges(from_task_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_task_edges_to ON task_edges(to_task_id);\n",
    "\n",
    "-- 3. Graph State Snapshots (for debugging and rollback)\n",
    "CREATE TABLE IF NOT EXISTS graph_snapshots (\n",
    "    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
    "    session_id UUID NOT NULL REFERENCES sessions(id) ON DELETE CASCADE,\n",
    "    snapshot_json JSONB NOT NULL,\n",
    "    node_name VARCHAR(50) NOT NULL,\n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n",
    "    \n",
    "    -- Metadata\n",
    "    agent_state JSONB DEFAULT '{}',\n",
    "    cost_summary JSONB DEFAULT '{}',\n",
    "    performance_metrics JSONB DEFAULT '{}'\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_graph_snapshots_session ON graph_snapshots(session_id, created_at DESC);\n",
    "CREATE INDEX IF NOT EXISTS idx_graph_snapshots_node ON graph_snapshots(node_name, created_at DESC);\n",
    "\n",
    "-- 4. Retrieval Performance Metrics\n",
    "CREATE TABLE IF NOT EXISTS retrieval_metrics (\n",
    "    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
    "    session_id UUID NOT NULL REFERENCES sessions(id) ON DELETE CASCADE,\n",
    "    query_text TEXT NOT NULL,\n",
    "    query_embedding vector(1536), -- OpenAI embedding dimension\n",
    "    k_initial INTEGER NOT NULL DEFAULT 10,\n",
    "    k_rerank INTEGER NOT NULL DEFAULT 5,\n",
    "    retrieval_latency_ms INTEGER,\n",
    "    rerank_latency_ms INTEGER,\n",
    "    usefulness_score DECIMAL(3,2) CHECK (usefulness_score BETWEEN 0.0 AND 1.0),\n",
    "    model_feedback JSONB DEFAULT '{}',\n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n",
    "    \n",
    "    -- Context metadata\n",
    "    context_tokens_before INTEGER,\n",
    "    context_tokens_after INTEGER,\n",
    "    compression_ratio DECIMAL(3,2),\n",
    "    documents_retrieved INTEGER,\n",
    "    documents_used INTEGER\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_retrieval_metrics_session ON retrieval_metrics(session_id, created_at DESC);\n",
    "CREATE INDEX IF NOT EXISTS idx_retrieval_metrics_usefulness ON retrieval_metrics(usefulness_score DESC);\n",
    "\n",
    "-- Vector similarity search index\n",
    "CREATE INDEX IF NOT EXISTS idx_retrieval_metrics_embedding ON retrieval_metrics \n",
    "USING ivfflat (query_embedding vector_cosine_ops) WITH (lists = 100);\n",
    "\n",
    "-- 5. Tool Performance and Capability Index\n",
    "CREATE TABLE IF NOT EXISTS tool_capability_index (\n",
    "    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
    "    tool_name VARCHAR(100) NOT NULL,\n",
    "    category VARCHAR(50) NOT NULL DEFAULT 'uncategorized',\n",
    "    version VARCHAR(20) DEFAULT '1.0.0',\n",
    "    \n",
    "    -- Performance metrics (rolling 30-day window)\n",
    "    success_rate_30d DECIMAL(5,2) DEFAULT 100.0,\n",
    "    avg_latency_ms DECIMAL(8,2) DEFAULT 0.0,\n",
    "    avg_cost_tokens DECIMAL(8,2) DEFAULT 0.0,\n",
    "    call_count_30d INTEGER DEFAULT 0,\n",
    "    \n",
    "    -- Capability metadata\n",
    "    safety_level VARCHAR(20) DEFAULT 'safe' CHECK (safety_level IN ('safe', 'caution', 'restricted')),\n",
    "    complexity_score INTEGER DEFAULT 5 CHECK (complexity_score BETWEEN 1 AND 10),\n",
    "    resource_intensity VARCHAR(20) DEFAULT 'light' CHECK (resource_intensity IN ('light', 'medium', 'heavy')),\n",
    "    \n",
    "    -- Timestamps\n",
    "    last_updated TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n",
    "    last_success TIMESTAMP WITH TIME ZONE,\n",
    "    last_failure TIMESTAMP WITH TIME ZONE,\n",
    "    \n",
    "    UNIQUE(tool_name, version)\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_tool_capability_name ON tool_capability_index(tool_name);\n",
    "CREATE INDEX IF NOT EXISTS idx_tool_capability_category ON tool_capability_index(category, success_rate_30d DESC);\n",
    "CREATE INDEX IF NOT EXISTS idx_tool_capability_performance ON tool_capability_index(success_rate_30d DESC, avg_latency_ms ASC);\n",
    "\n",
    "-- 6. Policy and Governance Decisions\n",
    "CREATE TABLE IF NOT EXISTS policy_decisions (\n",
    "    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),\n",
    "    session_id UUID REFERENCES sessions(id) ON DELETE CASCADE,\n",
    "    tool_name VARCHAR(100),\n",
    "    agent_name VARCHAR(50),\n",
    "    decision VARCHAR(20) NOT NULL CHECK (decision IN ('allow', 'deny', 'throttle', 'escalate', 'audit')),\n",
    "    policy_rule VARCHAR(100) NOT NULL,\n",
    "    reason TEXT,\n",
    "    \n",
    "    -- Budget and resource context\n",
    "    cost_limit_before INTEGER,\n",
    "    cost_limit_after INTEGER,\n",
    "    resource_usage_before JSONB DEFAULT '{}',\n",
    "    resource_usage_after JSONB DEFAULT '{}',\n",
    "    \n",
    "    created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),\n",
    "    \n",
    "    -- Audit trail\n",
    "    request_context JSONB DEFAULT '{}',\n",
    "    user_context JSONB DEFAULT '{}'\n",
    ");\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_policy_decisions_session ON policy_decisions(session_id, created_at DESC);\n",
    "CREATE INDEX IF NOT EXISTS idx_policy_decisions_tool ON policy_decisions(tool_name, decision, created_at DESC);\n",
    "CREATE INDEX IF NOT EXISTS idx_policy_decisions_agent ON policy_decisions(agent_name, created_at DESC);\n",
    "\n",
    "-- 7. Enhanced Message Storage (V2 extension)\n",
    "-- Extends existing conversation_messages with V2-specific metadata\n",
    "ALTER TABLE conversation_messages \n",
    "ADD COLUMN IF NOT EXISTS task_id UUID REFERENCES tasks_v2(id) ON DELETE SET NULL;\n",
    "\n",
    "ALTER TABLE conversation_messages \n",
    "ADD COLUMN IF NOT EXISTS graph_node VARCHAR(50);\n",
    "\n",
    "ALTER TABLE conversation_messages \n",
    "ADD COLUMN IF NOT EXISTS cost_tokens INTEGER DEFAULT 0;\n",
    "\n",
    "CREATE INDEX IF NOT EXISTS idx_conversation_messages_task ON conversation_messages(task_id);\n",
    "CREATE INDEX IF NOT EXISTS idx_conversation_messages_graph_node ON conversation_messages(graph_node, created_at DESC);\n",
    "\n",
    "-- 8. Create trigger for updated_at timestamps\n",
    "CREATE OR REPLACE FUNCTION update_updated_at_column()\n",
    "RETURNS TRIGGER AS $$\n",
    "BEGIN\n",
    "    NEW.updated_at = NOW();\n",
    "    RETURN NEW;\n",
    "END;\n",
    "$$ language 'plpgsql';\n",
    "\n",
    "CREATE TRIGGER update_tasks_v2_updated_at BEFORE UPDATE ON tasks_v2\n",
    "    FOR EACH ROW EXECUTE FUNCTION update_updated_at_column();\n",
    "\n",
    "COMMIT;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed7b44c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 263\u001b[0m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mawait\u001b[39;00m backfill\u001b[38;5;241m.\u001b[39mrun_backfill()\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 263\u001b[0m     \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;03mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m events\u001b[38;5;241m.\u001b[39m_get_running_loop() \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m coroutines\u001b[38;5;241m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ma coroutine was expected, got \u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "# V2 Data Backfill Script\n",
    "# File: scripts/migrate/backfill_v2_data.py\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime, timezone\n",
    "import psycopg\n",
    "import os\n",
    "from typing import Dict, List, Any\n",
    "\n",
    "DATABASE_URL = os.getenv(\"DATABASE_URL\")\n",
    "\n",
    "class V2DataBackfill:\n",
    "    \"\"\"Migrate V1 data to V2 schema structures\"\"\"\n",
    "    \n",
    "    def __init__(self, db_url: str):\n",
    "        self.db_url = db_url\n",
    "        self.migration_stats = {\n",
    "            'sessions_processed': 0,\n",
    "            'tasks_created': 0,\n",
    "            'task_edges_created': 0,\n",
    "            'tool_capabilities_indexed': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "    \n",
    "    async def run_backfill(self):\n",
    "        \"\"\"Execute complete V1 -> V2 data migration\"\"\"\n",
    "        print(\"üîÑ Starting Cartrita V2 data backfill...\")\n",
    "        \n",
    "        async with psycopg.AsyncConnection.connect(self.db_url) as conn:\n",
    "            # 1. Migrate sessions to task hierarchies\n",
    "            await self.backfill_task_hierarchies(conn)\n",
    "            \n",
    "            # 2. Build tool capability index from historical data\n",
    "            await self.build_tool_capability_index(conn)\n",
    "            \n",
    "            # 3. Generate initial graph snapshots for active sessions\n",
    "            await self.create_initial_graph_snapshots(conn)\n",
    "            \n",
    "            # 4. Populate retrieval metrics from conversation history\n",
    "            await self.backfill_retrieval_metrics(conn)\n",
    "            \n",
    "        self._print_migration_summary()\n",
    "    \n",
    "    async def backfill_task_hierarchies(self, conn):\n",
    "        \"\"\"Convert V1 conversation flows to V2 task hierarchies\"\"\"\n",
    "        print(\"üìã Converting conversations to task hierarchies...\")\n",
    "        \n",
    "        async with conn.cursor() as cur:\n",
    "            # Get all sessions with conversation messages\n",
    "            await cur.execute(\"\"\"\n",
    "                SELECT s.id, s.created_at, COUNT(cm.id) as message_count\n",
    "                FROM sessions s\n",
    "                LEFT JOIN conversation_messages cm ON s.id = cm.session_id\n",
    "                WHERE s.created_at >= NOW() - INTERVAL '30 days'\n",
    "                GROUP BY s.id, s.created_at\n",
    "                HAVING COUNT(cm.id) > 0\n",
    "                ORDER BY s.created_at DESC\n",
    "            \"\"\")\n",
    "            \n",
    "            sessions = await cur.fetchall()\n",
    "            \n",
    "            for session_id, created_at, msg_count in sessions:\n",
    "                await self.create_task_hierarchy_for_session(conn, session_id, created_at, msg_count)\n",
    "                self.migration_stats['sessions_processed'] += 1\n",
    "    \n",
    "    async def create_task_hierarchy_for_session(self, conn, session_id: str, created_at: datetime, msg_count: int):\n",
    "        \"\"\"Create root task and infer subtasks from conversation flow\"\"\"\n",
    "        \n",
    "        async with conn.cursor() as cur:\n",
    "            # Create root task\n",
    "            root_task_id = str(uuid.uuid4())\n",
    "            await cur.execute(\"\"\"\n",
    "                INSERT INTO tasks_v2 (id, session_id, parent_id, intent, spec_json, priority, status, created_at)\n",
    "                VALUES (%s, %s, NULL, %s, %s, %s, %s, %s)\n",
    "            \"\"\", (\n",
    "                root_task_id,\n",
    "                session_id,\n",
    "                \"root_conversation_objective\",\n",
    "                json.dumps({\"inferred_from\": \"v1_backfill\", \"message_count\": msg_count}),\n",
    "                5,\n",
    "                \"complete\",  # Historical sessions are complete\n",
    "                created_at\n",
    "            ))\n",
    "            \n",
    "            self.migration_stats['tasks_created'] += 1\n",
    "            \n",
    "            # Analyze conversation for subtasks (simplified heuristic)\n",
    "            await cur.execute(\"\"\"\n",
    "                SELECT role, content, created_at, metadata\n",
    "                FROM conversation_messages\n",
    "                WHERE session_id = %s\n",
    "                AND role IN ('assistant', 'tool')\n",
    "                ORDER BY created_at ASC\n",
    "            \"\"\", (session_id,))\n",
    "            \n",
    "            messages = await cur.fetchall()\n",
    "            \n",
    "            # Create subtasks based on tool calls and major assistant responses\n",
    "            for role, content, msg_created_at, metadata in messages:\n",
    "                if role == 'tool' or (role == 'assistant' and len(content) > 200):\n",
    "                    subtask_id = str(uuid.uuid4())\n",
    "                    intent = f\"{role}_response\" if role == 'assistant' else f\"tool_{metadata.get('tool_name', 'unknown')}\"\n",
    "                    \n",
    "                    await cur.execute(\"\"\"\n",
    "                        INSERT INTO tasks_v2 (id, session_id, parent_id, intent, spec_json, priority, status, created_at)\n",
    "                        VALUES (%s, %s, %s, %s, %s, %s, %s, %s)\n",
    "                    \"\"\", (\n",
    "                        subtask_id,\n",
    "                        session_id,\n",
    "                        root_task_id,\n",
    "                        intent,\n",
    "                        json.dumps({\"content_preview\": content[:100]}),\n",
    "                        3,\n",
    "                        \"complete\",\n",
    "                        msg_created_at\n",
    "                    ))\n",
    "                    \n",
    "                    self.migration_stats['tasks_created'] += 1\n",
    "    \n",
    "    async def build_tool_capability_index(self, conn):\n",
    "        \"\"\"Build tool performance index from historical tool invocations\"\"\"\n",
    "        print(\"üõ†Ô∏è  Building tool capability index...\")\n",
    "        \n",
    "        async with conn.cursor() as cur:\n",
    "            await cur.execute(\"\"\"\n",
    "                INSERT INTO tool_capability_index (\n",
    "                    tool_name, category, success_rate_30d, avg_latency_ms, \n",
    "                    avg_cost_tokens, call_count_30d, last_updated\n",
    "                )\n",
    "                SELECT\n",
    "                    COALESCE(ti.tool_name, 'unknown_tool') as tool_name,\n",
    "                    COALESCE(ti.category, 'uncategorized') as category,\n",
    "                    ROUND(\n",
    "                        (SUM(CASE WHEN ti.output_json->>'success' = 'true' THEN 1 ELSE 0 END)::decimal / \n",
    "                         NULLIF(COUNT(*), 0)) * 100, 2\n",
    "                    ) as success_rate_30d,\n",
    "                    ROUND(AVG(ti.latency_ms), 2) as avg_latency_ms,\n",
    "                    ROUND(AVG(COALESCE(ti.cost_tokens, 0)), 2) as avg_cost_tokens,\n",
    "                    COUNT(*) as call_count_30d,\n",
    "                    NOW() as last_updated\n",
    "                FROM tool_invocations ti\n",
    "                WHERE ti.created_at >= NOW() - INTERVAL '30 days'\n",
    "                GROUP BY ti.tool_name, ti.category\n",
    "                ON CONFLICT (tool_name, version) DO UPDATE SET\n",
    "                    success_rate_30d = EXCLUDED.success_rate_30d,\n",
    "                    avg_latency_ms = EXCLUDED.avg_latency_ms,\n",
    "                    avg_cost_tokens = EXCLUDED.avg_cost_tokens,\n",
    "                    call_count_30d = EXCLUDED.call_count_30d,\n",
    "                    last_updated = NOW()\n",
    "            \"\"\")\n",
    "            \n",
    "            tool_count = cur.rowcount\n",
    "            self.migration_stats['tool_capabilities_indexed'] = tool_count\n",
    "    \n",
    "    async def create_initial_graph_snapshots(self, conn):\n",
    "        \"\"\"Create initial graph snapshots for recent active sessions\"\"\"\n",
    "        print(\"üì∏ Creating initial graph snapshots...\")\n",
    "        \n",
    "        async with conn.cursor() as cur:\n",
    "            await cur.execute(\"\"\"\n",
    "                SELECT DISTINCT session_id \n",
    "                FROM tasks_v2 \n",
    "                WHERE created_at >= NOW() - INTERVAL '7 days'\n",
    "            \"\"\")\n",
    "            \n",
    "            recent_sessions = [row[0] for row in await cur.fetchall()]\n",
    "            \n",
    "            for session_id in recent_sessions:\n",
    "                await self.create_graph_snapshot(conn, session_id, \"migration_baseline\")\n",
    "    \n",
    "    async def create_graph_snapshot(self, conn, session_id: str, node_name: str):\n",
    "        \"\"\"Create a graph snapshot for a session\"\"\"\n",
    "        async with conn.cursor() as cur:\n",
    "            # Get task hierarchy for session\n",
    "            await cur.execute(\"\"\"\n",
    "                SELECT id, parent_id, intent, status, priority, created_at\n",
    "                FROM tasks_v2\n",
    "                WHERE session_id = %s\n",
    "                ORDER BY created_at ASC\n",
    "            \"\"\", (session_id,))\n",
    "            \n",
    "            tasks = await cur.fetchall()\n",
    "            \n",
    "            snapshot_data = {\n",
    "                \"tasks\": [\n",
    "                    {\n",
    "                        \"id\": str(task[0]),\n",
    "                        \"parent_id\": str(task[1]) if task[1] else None,\n",
    "                        \"intent\": task[2],\n",
    "                        \"status\": task[3],\n",
    "                        \"priority\": task[4],\n",
    "                        \"created_at\": task[5].isoformat()\n",
    "                    }\n",
    "                    for task in tasks\n",
    "                ],\n",
    "                \"migration_info\": {\n",
    "                    \"backfilled_at\": datetime.now(timezone.utc).isoformat(),\n",
    "                    \"task_count\": len(tasks)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            await cur.execute(\"\"\"\n",
    "                INSERT INTO graph_snapshots (session_id, snapshot_json, node_name)\n",
    "                VALUES (%s, %s, %s)\n",
    "            \"\"\", (session_id, json.dumps(snapshot_data), node_name))\n",
    "    \n",
    "    async def backfill_retrieval_metrics(self, conn):\n",
    "        \"\"\"Create initial retrieval metrics from conversation context\"\"\"\n",
    "        print(\"üîç Backfilling retrieval metrics...\")\n",
    "        \n",
    "        async with conn.cursor() as cur:\n",
    "            # This is a simplified backfill - in production you'd analyze actual RAG queries\n",
    "            await cur.execute(\"\"\"\n",
    "                INSERT INTO retrieval_metrics (\n",
    "                    session_id, query_text, k_initial, k_rerank, \n",
    "                    usefulness_score, model_feedback, context_tokens_before\n",
    "                )\n",
    "                SELECT DISTINCT\n",
    "                    cm.session_id,\n",
    "                    LEFT(cm.content, 200) as query_text,\n",
    "                    10 as k_initial,\n",
    "                    5 as k_rerank,\n",
    "                    0.7 as usefulness_score,  -- Default assumption\n",
    "                    '{\"source\": \"v1_backfill\"}' as model_feedback,\n",
    "                    LENGTH(cm.content) / 4 as context_tokens_before  -- Rough token estimate\n",
    "                FROM conversation_messages cm\n",
    "                WHERE cm.role = 'user'\n",
    "                AND cm.created_at >= NOW() - INTERVAL '30 days'\n",
    "                AND LENGTH(cm.content) > 50\n",
    "                LIMIT 1000  -- Don't overwhelm the backfill\n",
    "            \"\"\")\n",
    "    \n",
    "    def _print_migration_summary(self):\n",
    "        \"\"\"Print comprehensive migration summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"üéâ CARTRITA V2 MIGRATION COMPLETE!\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"üìä Sessions Processed: {self.migration_stats['sessions_processed']}\")\n",
    "        print(f\"üìã Tasks Created: {self.migration_stats['tasks_created']}\")\n",
    "        print(f\"üîó Task Edges Created: {self.migration_stats['task_edges_created']}\")\n",
    "        print(f\"üõ†Ô∏è  Tools Indexed: {self.migration_stats['tool_capabilities_indexed']}\")\n",
    "        \n",
    "        if self.migration_stats['errors']:\n",
    "            print(f\"‚ö†Ô∏è  Errors Encountered: {len(self.migration_stats['errors'])}\")\n",
    "            for error in self.migration_stats['errors'][:5]:  # Show first 5 errors\n",
    "                print(f\"   - {error}\")\n",
    "        else:\n",
    "            print(\"‚úÖ No errors encountered!\")\n",
    "        \n",
    "        print(\"\\nüöÄ Ready for V2 orchestrator deployment!\")\n",
    "\n",
    "async def main():\n",
    "    if not DATABASE_URL:\n",
    "        print(\"‚ùå DATABASE_URL environment variable required\")\n",
    "        return\n",
    "    \n",
    "    backfill = V2DataBackfill(DATABASE_URL)\n",
    "    await backfill.run_backfill()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13748bd4",
   "metadata": {},
   "source": [
    "# 3. Core Architecture Components\n",
    "\n",
    "This section implements the foundational V2 architecture components that form the backbone of the hybrid multi-agent system. Each component is designed for high availability, observability, and seamless integration.\n",
    "\n",
    "## 3.1 Architecture Component Overview\n",
    "\n",
    "### V2 Service Architecture:\n",
    "1. **Gateway Layer (Node.js)** - Real-time I/O, WebSocket streaming, session management\n",
    "2. **Orchestrator (Python)** - LangGraph state machine controlling agent workflows  \n",
    "3. **Agent Microservices (Python)** - Specialized agents with encapsulated capabilities\n",
    "4. **Tool Mesh (Hybrid)** - MCP servers bridging Node.js and Python tools\n",
    "5. **Event Spine (NATS)** - Message bus enabling reactive event-driven coordination\n",
    "6. **Memory & RAG Layer** - Intelligent context management and retrieval\n",
    "7. **Governance & Policy** - Budget enforcement, safety filters, compliance\n",
    "\n",
    "### Key Design Principles:\n",
    "- **Separation of Concerns**: Each layer has distinct responsibilities\n",
    "- **Event-Driven**: All coordination happens through message passing\n",
    "- **Fault Tolerant**: Graceful degradation with automatic recovery\n",
    "- **Observable**: Complete tracing and metrics for all operations\n",
    "- **Scalable**: Horizontal scaling with stateless services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d14da9",
   "metadata": {
    "vscode": {
     "languageId": "javascript"
    }
   },
   "outputs": [],
   "source": [
    "// File: gateway-node/src/index.ts\n",
    "// Cartrita V2 Node.js Gateway - Real-time I/O and WebSocket streaming\n",
    "\n",
    "import Fastify from 'fastify';\n",
    "import WebSocket from 'ws';\n",
    "import { connect as natsConnect, NatsConnection } from 'nats';\n",
    "import { randomUUID } from 'crypto';\n",
    "import { createClient } from 'redis';\n",
    "import { trace, context, SpanStatusCode } from '@opentelemetry/api';\n",
    "\n",
    "interface SessionContext {\n",
    "  sessionId: string;\n",
    "  userId?: string;\n",
    "  websocket?: WebSocket;\n",
    "  lastActivity: Date;\n",
    "  metadata: Record<string, any>;\n",
    "}\n",
    "\n",
    "class CartritaV2Gateway {\n",
    "  private fastify: any;\n",
    "  private nats!: NatsConnection;\n",
    "  private redis: any;\n",
    "  private activeSessions = new Map<string, SessionContext>();\n",
    "  private tracer = trace.getTracer('cartrita-v2-gateway');\n",
    "\n",
    "  constructor() {\n",
    "    this.fastify = Fastify({\n",
    "      logger: {\n",
    "        level: 'info',\n",
    "        transport: {\n",
    "          target: 'pino-pretty',\n",
    "          options: { colorize: true }\n",
    "        }\n",
    "      }\n",
    "    });\n",
    "    \n",
    "    this.redis = createClient({\n",
    "      url: process.env.REDIS_URL || 'redis://localhost:6379'\n",
    "    });\n",
    "  }\n",
    "\n",
    "  async start() {\n",
    "    await this.initializeConnections();\n",
    "    await this.setupRoutes();\n",
    "    await this.setupWebSocketServer();\n",
    "    \n",
    "    const port = parseInt(process.env.PORT || '3000');\n",
    "    await this.fastify.listen({ port, host: '0.0.0.0' });\n",
    "    \n",
    "    console.log(`üöÄ Cartrita V2 Gateway running on port ${port}`);\n",
    "    console.log(`üî• Ready to orchestrate the future of AI!`);\n",
    "  }\n",
    "\n",
    "  private async initializeConnections() {\n",
    "    // Connect to NATS event spine\n",
    "    this.nats = await natsConnect({\n",
    "      servers: process.env.NATS_URL || 'nats://localhost:4222',\n",
    "      name: 'cartrita-v2-gateway',\n",
    "      maxReconnectAttempts: 10,\n",
    "      reconnectTimeWait: 2000\n",
    "    });\n",
    "\n",
    "    // Connect to Redis for session state\n",
    "    await this.redis.connect();\n",
    "\n",
    "    console.log('‚úÖ Connected to NATS and Redis');\n",
    "  }\n",
    "\n",
    "  private async setupRoutes() {\n",
    "    // Health check\n",
    "    this.fastify.get('/health', async () => ({\n",
    "      status: 'healthy',\n",
    "      version: '2.0.0',\n",
    "      components: {\n",
    "        nats: this.nats.isClosed() ? 'disconnected' : 'connected',\n",
    "        redis: this.redis.isReady ? 'connected' : 'disconnected',\n",
    "        sessions: this.activeSessions.size\n",
    "      }\n",
    "    }));\n",
    "\n",
    "    // Create new session\n",
    "    this.fastify.post('/api/v2/sessions', async (request: any, reply: any) => {\n",
    "      return this.tracer.startActiveSpan('create-session', async (span) => {\n",
    "        try {\n",
    "          const sessionId = randomUUID();\n",
    "          const sessionContext: SessionContext = {\n",
    "            sessionId,\n",
    "            userId: request.body?.userId,\n",
    "            lastActivity: new Date(),\n",
    "            metadata: request.body?.metadata || {}\n",
    "          };\n",
    "\n",
    "          // Store in Redis with TTL\n",
    "          await this.redis.setex(\n",
    "            `session:${sessionId}`,\n",
    "            3600, // 1 hour TTL\n",
    "            JSON.stringify(sessionContext)\n",
    "          );\n",
    "\n",
    "          this.activeSessions.set(sessionId, sessionContext);\n",
    "\n",
    "          // Publish session created event\n",
    "          await this.nats.publish(`session.${sessionId}.created`, \n",
    "            JSON.stringify({\n",
    "              sessionId,\n",
    "              userId: sessionContext.userId,\n",
    "              createdAt: sessionContext.lastActivity.toISOString(),\n",
    "              metadata: sessionContext.metadata\n",
    "            })\n",
    "          );\n",
    "\n",
    "          span.setStatus({ code: SpanStatusCode.OK });\n",
    "          \n",
    "          return {\n",
    "            sessionId,\n",
    "            websocketUrl: `/ws/${sessionId}`,\n",
    "            expiresAt: new Date(Date.now() + 3600000).toISOString()\n",
    "          };\n",
    "        } catch (error) {\n",
    "          span.recordException(error as Error);\n",
    "          span.setStatus({ code: SpanStatusCode.ERROR });\n",
    "          throw error;\n",
    "        } finally {\n",
    "          span.end();\n",
    "        }\n",
    "      });\n",
    "    });\n",
    "\n",
    "    // Send message to session\n",
    "    this.fastify.post('/api/v2/sessions/:sessionId/messages', async (request: any, reply: any) => {\n",
    "      return this.tracer.startActiveSpan('send-message', async (span) => {\n",
    "        try {\n",
    "          const { sessionId } = request.params;\n",
    "          const { content, role = 'user', metadata = {} } = request.body;\n",
    "\n",
    "          // Validate session exists\n",
    "          const sessionContext = await this.getSessionContext(sessionId);\n",
    "          if (!sessionContext) {\n",
    "            reply.status(404);\n",
    "            return { error: 'Session not found' };\n",
    "          }\n",
    "\n",
    "          const messageId = randomUUID();\n",
    "          const message = {\n",
    "            id: messageId,\n",
    "            sessionId,\n",
    "            content,\n",
    "            role,\n",
    "            metadata,\n",
    "            timestamp: new Date().toISOString()\n",
    "          };\n",
    "\n",
    "          // Publish to orchestrator\n",
    "          await this.nats.publish(`session.${sessionId}.input`, JSON.stringify(message));\n",
    "\n",
    "          // Update session activity\n",
    "          await this.updateSessionActivity(sessionId);\n",
    "\n",
    "          span.setAttributes({\n",
    "            sessionId,\n",
    "            messageId,\n",
    "            role,\n",
    "            contentLength: content.length\n",
    "          });\n",
    "\n",
    "          return { messageId, status: 'queued' };\n",
    "        } catch (error) {\n",
    "          span.recordException(error as Error);\n",
    "          span.setStatus({ code: SpanStatusCode.ERROR });\n",
    "          throw error;\n",
    "        } finally {\n",
    "          span.end();\n",
    "        }\n",
    "      });\n",
    "    });\n",
    "\n",
    "    // Get session status\n",
    "    this.fastify.get('/api/v2/sessions/:sessionId/status', async (request: any) => {\n",
    "      const { sessionId } = request.params;\n",
    "      const sessionContext = await this.getSessionContext(sessionId);\n",
    "      \n",
    "      if (!sessionContext) {\n",
    "        return { error: 'Session not found' };\n",
    "      }\n",
    "\n",
    "      return {\n",
    "        sessionId,\n",
    "        status: 'active',\n",
    "        lastActivity: sessionContext.lastActivity,\n",
    "        hasWebSocket: !!sessionContext.websocket\n",
    "      };\n",
    "    });\n",
    "  }\n",
    "\n",
    "  private async setupWebSocketServer() {\n",
    "    const wss = new WebSocket.Server({ \n",
    "      port: parseInt(process.env.WS_PORT || '3001'),\n",
    "      path: '/ws'\n",
    "    });\n",
    "\n",
    "    wss.on('connection', (ws, request) => {\n",
    "      const url = new URL(request.url!, 'ws://localhost');\n",
    "      const sessionId = url.pathname.split('/').pop();\n",
    "\n",
    "      if (!sessionId) {\n",
    "        ws.close(1008, 'Session ID required');\n",
    "        return;\n",
    "      }\n",
    "\n",
    "      this.handleWebSocketConnection(ws, sessionId);\n",
    "    });\n",
    "\n",
    "    // Subscribe to orchestrator outputs for streaming\n",
    "    const sub = this.nats.subscribe('session.*.output');\n",
    "    for await (const msg of sub) {\n",
    "      const subject = msg.subject;\n",
    "      const sessionId = subject.split('.')[1];\n",
    "      const data = JSON.parse(msg.data.toString());\n",
    "      \n",
    "      await this.streamToSession(sessionId, data);\n",
    "    }\n",
    "  }\n",
    "\n",
    "  private async handleWebSocketConnection(ws: WebSocket, sessionId: string) {\n",
    "    console.log(`üîå WebSocket connected for session ${sessionId}`);\n",
    "\n",
    "    const sessionContext = await this.getSessionContext(sessionId);\n",
    "    if (!sessionContext) {\n",
    "      ws.close(1008, 'Invalid session');\n",
    "      return;\n",
    "    }\n",
    "\n",
    "    // Update session with WebSocket reference\n",
    "    sessionContext.websocket = ws;\n",
    "    this.activeSessions.set(sessionId, sessionContext);\n",
    "\n",
    "    ws.on('message', async (data) => {\n",
    "      try {\n",
    "        const message = JSON.parse(data.toString());\n",
    "        \n",
    "        // Forward to orchestrator\n",
    "        await this.nats.publish(`session.${sessionId}.input`, JSON.stringify({\n",
    "          ...message,\n",
    "          sessionId,\n",
    "          timestamp: new Date().toISOString()\n",
    "        }));\n",
    "\n",
    "        await this.updateSessionActivity(sessionId);\n",
    "      } catch (error) {\n",
    "        console.error(`WebSocket message error for ${sessionId}:`, error);\n",
    "        ws.send(JSON.stringify({ error: 'Invalid message format' }));\n",
    "      }\n",
    "    });\n",
    "\n",
    "    ws.on('close', () => {\n",
    "      console.log(`üîå WebSocket disconnected for session ${sessionId}`);\n",
    "      const session = this.activeSessions.get(sessionId);\n",
    "      if (session) {\n",
    "        session.websocket = undefined;\n",
    "        this.activeSessions.set(sessionId, session);\n",
    "      }\n",
    "    });\n",
    "\n",
    "    ws.on('error', (error) => {\n",
    "      console.error(`WebSocket error for ${sessionId}:`, error);\n",
    "    });\n",
    "\n",
    "    // Send connection confirmation\n",
    "    ws.send(JSON.stringify({\n",
    "      type: 'connection',\n",
    "      sessionId,\n",
    "      status: 'connected',\n",
    "      timestamp: new Date().toISOString()\n",
    "    }));\n",
    "  }\n",
    "\n",
    "  private async streamToSession(sessionId: string, data: any) {\n",
    "    const session = this.activeSessions.get(sessionId);\n",
    "    if (session?.websocket && session.websocket.readyState === WebSocket.OPEN) {\n",
    "      session.websocket.send(JSON.stringify(data));\n",
    "    }\n",
    "  }\n",
    "\n",
    "  private async getSessionContext(sessionId: string): Promise<SessionContext | null> {\n",
    "    // Try memory first\n",
    "    let session = this.activeSessions.get(sessionId);\n",
    "    if (session) return session;\n",
    "\n",
    "    // Fall back to Redis\n",
    "    const cached = await this.redis.get(`session:${sessionId}`);\n",
    "    if (cached) {\n",
    "      session = JSON.parse(cached);\n",
    "      this.activeSessions.set(sessionId, session!);\n",
    "      return session!;\n",
    "    }\n",
    "\n",
    "    return null;\n",
    "  }\n",
    "\n",
    "  private async updateSessionActivity(sessionId: string) {\n",
    "    const session = this.activeSessions.get(sessionId);\n",
    "    if (session) {\n",
    "      session.lastActivity = new Date();\n",
    "      this.activeSessions.set(sessionId, session);\n",
    "      \n",
    "      // Update Redis cache\n",
    "      await this.redis.setex(\n",
    "        `session:${sessionId}`,\n",
    "        3600,\n",
    "        JSON.stringify(session)\n",
    "      );\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "// Start the gateway\n",
    "const gateway = new CartritaV2Gateway();\n",
    "gateway.start().catch(console.error);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b32b959b",
   "metadata": {},
   "source": [
    "# 4. OpenAI Integration Layer\n",
    "\n",
    "This section implements the unified OpenAI integration layer for Cartrita V2, providing intelligent model selection, cost optimization, structured output enforcement, and comprehensive token tracking across all agent operations.\n",
    "\n",
    "## 4.1 Model Tiering Strategy\n",
    "\n",
    "### V2 Model Selection Architecture:\n",
    "- **High-Stakes Reasoning**: `gpt-4o` for critical planning and final synthesis\n",
    "- **Fast Iteration**: `gpt-4o-mini` for decomposition, classification, and intermediate steps  \n",
    "- **Structured Extraction**: JSON mode + function calling for deterministic outputs\n",
    "- **Cost Optimization**: Automatic model downshifting based on budget constraints\n",
    "\n",
    "### Key Features:\n",
    "1. **Adaptive Model Selection** - Budget-aware automatic fallback\n",
    "2. **Retry Logic with Exponential Backoff** - Handles rate limits gracefully\n",
    "3. **Token Tracking** - Real-time cost monitoring and budget enforcement\n",
    "4. **Structured Output Validation** - Schema enforcement with repair prompts\n",
    "5. **Streaming Support** - Token-by-token delivery for user experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f893af3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: shared/llm/openai_wrapper.py\n",
    "# Cartrita V2 Unified OpenAI Integration Layer\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "from typing import Dict, Any, List, Optional, AsyncGenerator, Union\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema import BaseMessage, HumanMessage, SystemMessage, AIMessage\n",
    "from openai import AsyncOpenAI, RateLimitError, APIError\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential, retry_if_exception_type\n",
    "from opentelemetry import trace\n",
    "import tiktoken\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "tracer = trace.get_tracer(\"cartrita-v2-openai\")\n",
    "\n",
    "class ModelTier(Enum):\n",
    "    \"\"\"Model performance and cost tiers\"\"\"\n",
    "    PREMIUM = \"gpt-4o\"\n",
    "    STANDARD = \"gpt-4o-mini\" \n",
    "    FALLBACK = \"gpt-3.5-turbo\"\n",
    "\n",
    "@dataclass\n",
    "class TokenUsage:\n",
    "    \"\"\"Token usage tracking\"\"\"\n",
    "    prompt_tokens: int = 0\n",
    "    completion_tokens: int = 0\n",
    "    total_tokens: int = 0\n",
    "    estimated_cost: float = 0.0\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    \"\"\"Model configuration and pricing\"\"\"\n",
    "    name: str\n",
    "    cost_per_input_token: float\n",
    "    cost_per_output_token: float\n",
    "    context_limit: int\n",
    "    supports_functions: bool = True\n",
    "    supports_json_mode: bool = True\n",
    "\n",
    "class OpenAIWrapper:\n",
    "    \"\"\"\n",
    "    Unified OpenAI wrapper for Cartrita V2 with intelligent model selection,\n",
    "    cost tracking, retry logic, and structured output enforcement.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Model configurations with 2025 pricing\n",
    "    MODEL_CONFIGS = {\n",
    "        ModelTier.PREMIUM: ModelConfig(\n",
    "            name=\"gpt-4o\",\n",
    "            cost_per_input_token=0.0025 / 1000,\n",
    "            cost_per_output_token=0.01 / 1000,\n",
    "            context_limit=128000\n",
    "        ),\n",
    "        ModelTier.STANDARD: ModelConfig(\n",
    "            name=\"gpt-4o-mini\",\n",
    "            cost_per_input_token=0.00015 / 1000,\n",
    "            cost_per_output_token=0.0006 / 1000,\n",
    "            context_limit=128000\n",
    "        ),\n",
    "        ModelTier.FALLBACK: ModelConfig(\n",
    "            name=\"gpt-3.5-turbo\",\n",
    "            cost_per_input_token=0.0005 / 1000,\n",
    "            cost_per_output_token=0.0015 / 1000,\n",
    "            context_limit=16000\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    def __init__(\n",
    "        self, \n",
    "        default_tier: ModelTier = ModelTier.STANDARD,\n",
    "        temperature: float = 0.2,\n",
    "        max_retries: int = 3,\n",
    "        api_key: Optional[str] = None\n",
    "    ):\n",
    "        self.default_tier = default_tier\n",
    "        self.temperature = temperature\n",
    "        self.max_retries = max_retries\n",
    "        \n",
    "        # Initialize OpenAI clients\n",
    "        self.client = AsyncOpenAI(api_key=api_key)\n",
    "        self.langchain_clients = {\n",
    "            tier: ChatOpenAI(\n",
    "                model=config.name,\n",
    "                temperature=temperature,\n",
    "                api_key=api_key,\n",
    "                streaming=True\n",
    "            )\n",
    "            for tier, config in self.MODEL_CONFIGS.items()\n",
    "        }\n",
    "        \n",
    "        # Token counting\n",
    "        self.encodings = {\n",
    "            tier.value: tiktoken.encoding_for_model(config.name)\n",
    "            for tier, config in self.MODEL_CONFIGS.items()\n",
    "        }\n",
    "        \n",
    "        # Session tracking\n",
    "        self.session_usage: Dict[str, TokenUsage] = {}\n",
    "        self.session_budgets: Dict[str, int] = {}\n",
    "    \n",
    "    def set_session_budget(self, session_id: str, token_budget: int):\n",
    "        \"\"\"Set token budget for a session\"\"\"\n",
    "        self.session_budgets[session_id] = token_budget\n",
    "        if session_id not in self.session_usage:\n",
    "            self.session_usage[session_id] = TokenUsage()\n",
    "    \n",
    "    def get_session_usage(self, session_id: str) -> TokenUsage:\n",
    "        \"\"\"Get current token usage for session\"\"\"\n",
    "        return self.session_usage.get(session_id, TokenUsage())\n",
    "    \n",
    "    def select_model_tier(self, session_id: str, preferred_tier: ModelTier) -> ModelTier:\n",
    "        \"\"\"Select appropriate model tier based on budget constraints\"\"\"\n",
    "        if session_id not in self.session_budgets:\n",
    "            return preferred_tier\n",
    "        \n",
    "        budget = self.session_budgets[session_id]\n",
    "        used = self.session_usage[session_id].total_tokens\n",
    "        remaining = budget - used\n",
    "        \n",
    "        # Estimate tokens needed for this request (conservative)\n",
    "        estimated_request_tokens = 1000  # Default estimate\n",
    "        \n",
    "        if remaining < estimated_request_tokens:\n",
    "            # Budget tight - use fallback\n",
    "            logger.warning(f\"Budget constraint for {session_id}: switching to {ModelTier.FALLBACK}\")\n",
    "            return ModelTier.FALLBACK\n",
    "        elif remaining < estimated_request_tokens * 3:\n",
    "            # Budget moderate - use standard\n",
    "            if preferred_tier == ModelTier.PREMIUM:\n",
    "                logger.info(f\"Budget optimization for {session_id}: using {ModelTier.STANDARD}\")\n",
    "                return ModelTier.STANDARD\n",
    "        \n",
    "        return preferred_tier\n",
    "    \n",
    "    def count_tokens(self, text: str, model_tier: ModelTier) -> int:\n",
    "        \"\"\"Count tokens for given text and model\"\"\"\n",
    "        try:\n",
    "            encoding = self.encodings.get(model_tier.value)\n",
    "            if encoding:\n",
    "                return len(encoding.encode(text))\n",
    "            return len(text) // 4  # Rough approximation\n",
    "        except Exception:\n",
    "            return len(text) // 4  # Fallback approximation\n",
    "    \n",
    "    def estimate_cost(self, prompt_tokens: int, completion_tokens: int, model_tier: ModelTier) -> float:\n",
    "        \"\"\"Estimate cost for token usage\"\"\"\n",
    "        config = self.MODEL_CONFIGS[model_tier]\n",
    "        return (\n",
    "            prompt_tokens * config.cost_per_input_token + \n",
    "            completion_tokens * config.cost_per_output_token\n",
    "        )\n",
    "    \n",
    "    @retry(\n",
    "        reraise=True,\n",
    "        stop=stop_after_attempt(5),\n",
    "        wait=wait_exponential(multiplier=0.5, min=0.5, max=8),\n",
    "        retry=retry_if_exception_type((RateLimitError, APIError))\n",
    "    )\n",
    "    async def complete(\n",
    "        self,\n",
    "        messages: List[BaseMessage],\n",
    "        session_id: Optional[str] = None,\n",
    "        preferred_tier: ModelTier = None,\n",
    "        tools: Optional[List[Dict[str, Any]]] = None,\n",
    "        tool_choice: Optional[str] = None,\n",
    "        json_mode: bool = False,\n",
    "        stream: bool = False\n",
    "    ) -> Union[AIMessage, AsyncGenerator[str, None]]:\n",
    "        \"\"\"\n",
    "        Complete chat with intelligent model selection and comprehensive tracking\n",
    "        \"\"\"\n",
    "        \n",
    "        with tracer.start_as_current_span(\"openai-complete\") as span:\n",
    "            # Select model tier\n",
    "            tier = preferred_tier or self.default_tier\n",
    "            if session_id:\n",
    "                tier = self.select_model_tier(session_id, tier)\n",
    "            \n",
    "            config = self.MODEL_CONFIGS[tier]\n",
    "            \n",
    "            # Convert messages to OpenAI format\n",
    "            formatted_messages = self._format_messages(messages)\n",
    "            \n",
    "            # Count input tokens\n",
    "            prompt_text = \"\\n\".join([msg.get(\"content\", \"\") for msg in formatted_messages])\n",
    "            prompt_tokens = self.count_tokens(prompt_text, tier)\n",
    "            \n",
    "            # Prepare request\n",
    "            request_data = {\n",
    "                \"model\": config.name,\n",
    "                \"messages\": formatted_messages,\n",
    "                \"temperature\": self.temperature,\n",
    "                \"stream\": stream\n",
    "            }\n",
    "            \n",
    "            if tools:\n",
    "                request_data[\"tools\"] = tools\n",
    "                if tool_choice:\n",
    "                    request_data[\"tool_choice\"] = tool_choice\n",
    "            \n",
    "            if json_mode:\n",
    "                request_data[\"response_format\"] = {\"type\": \"json_object\"}\n",
    "            \n",
    "            # Add span attributes\n",
    "            span.set_attributes({\n",
    "                \"model\": config.name,\n",
    "                \"session_id\": session_id or \"unknown\",\n",
    "                \"prompt_tokens\": prompt_tokens,\n",
    "                \"json_mode\": json_mode,\n",
    "                \"has_tools\": bool(tools)\n",
    "            })\n",
    "            \n",
    "            try:\n",
    "                start_time = time.time()\n",
    "                \n",
    "                if stream:\n",
    "                    return self._stream_completion(request_data, session_id, tier, prompt_tokens, span)\n",
    "                else:\n",
    "                    response = await self.client.chat.completions.create(**request_data)\n",
    "                    return await self._process_completion_response(\n",
    "                        response, session_id, tier, prompt_tokens, start_time, span\n",
    "                    )\n",
    "                    \n",
    "            except Exception as e:\n",
    "                span.record_exception(e)\n",
    "                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))\n",
    "                logger.error(f\"OpenAI completion failed: {e}\")\n",
    "                raise\n",
    "    \n",
    "    async def _stream_completion(\n",
    "        self,\n",
    "        request_data: Dict[str, Any], \n",
    "        session_id: Optional[str],\n",
    "        tier: ModelTier,\n",
    "        prompt_tokens: int,\n",
    "        span\n",
    "    ) -> AsyncGenerator[str, None]:\n",
    "        \"\"\"Handle streaming completion\"\"\"\n",
    "        \n",
    "        async with await self.client.chat.completions.create(**request_data) as stream:\n",
    "            collected_content = \"\"\n",
    "            \n",
    "            async for chunk in stream:\n",
    "                if chunk.choices and chunk.choices[0].delta.content:\n",
    "                    content = chunk.choices[0].delta.content\n",
    "                    collected_content += content\n",
    "                    yield content\n",
    "            \n",
    "            # Track completion tokens\n",
    "            completion_tokens = self.count_tokens(collected_content, tier)\n",
    "            self._update_session_usage(session_id, prompt_tokens, completion_tokens, tier)\n",
    "            \n",
    "            span.set_attributes({\n",
    "                \"completion_tokens\": completion_tokens,\n",
    "                \"total_tokens\": prompt_tokens + completion_tokens\n",
    "            })\n",
    "    \n",
    "    async def _process_completion_response(\n",
    "        self,\n",
    "        response,\n",
    "        session_id: Optional[str],\n",
    "        tier: ModelTier,\n",
    "        prompt_tokens: int,\n",
    "        start_time: float,\n",
    "        span\n",
    "    ) -> AIMessage:\n",
    "        \"\"\"Process non-streaming completion response\"\"\"\n",
    "        \n",
    "        latency_ms = (time.time() - start_time) * 1000\n",
    "        \n",
    "        # Extract usage\n",
    "        usage = response.usage\n",
    "        completion_tokens = usage.completion_tokens if usage else 0\n",
    "        total_tokens = usage.total_tokens if usage else prompt_tokens + completion_tokens\n",
    "        \n",
    "        # Update session tracking\n",
    "        self._update_session_usage(session_id, prompt_tokens, completion_tokens, tier)\n",
    "        \n",
    "        # Create response message\n",
    "        message_content = response.choices[0].message.content\n",
    "        tool_calls = getattr(response.choices[0].message, 'tool_calls', None)\n",
    "        \n",
    "        additional_kwargs = {}\n",
    "        if tool_calls:\n",
    "            additional_kwargs['tool_calls'] = [\n",
    "                {\n",
    "                    'id': call.id,\n",
    "                    'type': call.type,\n",
    "                    'function': {\n",
    "                        'name': call.function.name,\n",
    "                        'arguments': call.function.arguments\n",
    "                    }\n",
    "                }\n",
    "                for call in tool_calls\n",
    "            ]\n",
    "        \n",
    "        # Update span\n",
    "        span.set_attributes({\n",
    "            \"completion_tokens\": completion_tokens,\n",
    "            \"total_tokens\": total_tokens,\n",
    "            \"latency_ms\": latency_ms,\n",
    "            \"has_tool_calls\": bool(tool_calls)\n",
    "        })\n",
    "        \n",
    "        return AIMessage(\n",
    "            content=message_content or \"\",\n",
    "            additional_kwargs=additional_kwargs\n",
    "        )\n",
    "    \n",
    "    def _update_session_usage(\n",
    "        self, \n",
    "        session_id: Optional[str], \n",
    "        prompt_tokens: int, \n",
    "        completion_tokens: int, \n",
    "        tier: ModelTier\n",
    "    ):\n",
    "        \"\"\"Update session token usage tracking\"\"\"\n",
    "        if not session_id:\n",
    "            return\n",
    "        \n",
    "        if session_id not in self.session_usage:\n",
    "            self.session_usage[session_id] = TokenUsage()\n",
    "        \n",
    "        usage = self.session_usage[session_id]\n",
    "        usage.prompt_tokens += prompt_tokens\n",
    "        usage.completion_tokens += completion_tokens\n",
    "        usage.total_tokens += prompt_tokens + completion_tokens\n",
    "        usage.estimated_cost += self.estimate_cost(prompt_tokens, completion_tokens, tier)\n",
    "        \n",
    "        # Log if approaching budget\n",
    "        if session_id in self.session_budgets:\n",
    "            budget = self.session_budgets[session_id]\n",
    "            usage_pct = (usage.total_tokens / budget) * 100\n",
    "            \n",
    "            if usage_pct > 80:\n",
    "                logger.warning(f\"Session {session_id} at {usage_pct:.1f}% of token budget\")\n",
    "    \n",
    "    def _format_messages(self, messages: List[BaseMessage]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Convert LangChain messages to OpenAI format\"\"\"\n",
    "        formatted = []\n",
    "        \n",
    "        for msg in messages:\n",
    "            if isinstance(msg, HumanMessage):\n",
    "                formatted.append({\"role\": \"user\", \"content\": msg.content})\n",
    "            elif isinstance(msg, SystemMessage):\n",
    "                formatted.append({\"role\": \"system\", \"content\": msg.content})\n",
    "            elif isinstance(msg, AIMessage):\n",
    "                msg_dict = {\"role\": \"assistant\", \"content\": msg.content}\n",
    "                if msg.additional_kwargs.get(\"tool_calls\"):\n",
    "                    msg_dict[\"tool_calls\"] = msg.additional_kwargs[\"tool_calls\"]\n",
    "                formatted.append(msg_dict)\n",
    "        \n",
    "        return formatted\n",
    "\n",
    "# Validation helpers for structured outputs\n",
    "def safe_json_extract(text: str) -> Dict[str, Any]:\n",
    "    \"\"\"Safely extract JSON from LLM response with fallback\"\"\"\n",
    "    import re\n",
    "    \n",
    "    try:\n",
    "        return json.loads(text)\n",
    "    except json.JSONDecodeError:\n",
    "        # Try to find JSON within text\n",
    "        json_match = re.search(r'\\{.*\\}', text, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                return json.loads(json_match.group(0))\n",
    "            except json.JSONDecodeError:\n",
    "                pass\n",
    "        \n",
    "        return {\"_raw\": text, \"_error\": \"parse_failed\"}\n",
    "\n",
    "async def validate_structured_output(\n",
    "    wrapper: OpenAIWrapper,\n",
    "    messages: List[BaseMessage],\n",
    "    expected_schema: Dict[str, Any],\n",
    "    session_id: str,\n",
    "    max_retries: int = 2\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"Validate and repair structured outputs with schema enforcement\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries + 1):\n",
    "        try:\n",
    "            response = await wrapper.complete(\n",
    "                messages=messages,\n",
    "                session_id=session_id,\n",
    "                json_mode=True\n",
    "            )\n",
    "            \n",
    "            result = safe_json_extract(response.content)\n",
    "            \n",
    "            if \"_error\" not in result:\n",
    "                # TODO: Add pydantic schema validation here\n",
    "                return result\n",
    "            \n",
    "            # Repair prompt for next attempt\n",
    "            if attempt < max_retries:\n",
    "                repair_msg = SystemMessage(content=f\"The previous response had invalid JSON. Please return ONLY valid JSON matching the schema. Error: {result.get('_error')}\")\n",
    "                messages.append(repair_msg)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Structured output validation failed (attempt {attempt + 1}): {e}\")\n",
    "            if attempt == max_retries:\n",
    "                raise\n",
    "    \n",
    "    return {\"_error\": \"max_retries_exceeded\"}\n",
    "\n",
    "# Usage examples and factory functions\n",
    "def create_planning_wrapper() -> OpenAIWrapper:\n",
    "    \"\"\"Create wrapper optimized for planning tasks\"\"\"\n",
    "    return OpenAIWrapper(\n",
    "        default_tier=ModelTier.PREMIUM,\n",
    "        temperature=0.1\n",
    "    )\n",
    "\n",
    "def create_execution_wrapper() -> OpenAIWrapper:\n",
    "    \"\"\"Create wrapper optimized for execution tasks\"\"\"\n",
    "    return OpenAIWrapper(\n",
    "        default_tier=ModelTier.STANDARD,\n",
    "        temperature=0.2\n",
    "    )\n",
    "\n",
    "def create_creative_wrapper() -> OpenAIWrapper:\n",
    "    \"\"\"Create wrapper optimized for creative tasks\"\"\"\n",
    "    return OpenAIWrapper(\n",
    "        default_tier=ModelTier.STANDARD,\n",
    "        temperature=0.7\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224e3357",
   "metadata": {},
   "source": [
    "# 5. MCP (Model Context Protocol) Implementation\n",
    "\n",
    "This section implements the Model Context Protocol layer that standardizes tool registration and invocation across Node.js and Python services, enabling seamless cross-language tool integration with comprehensive schema validation and streaming support.\n",
    "\n",
    "## 5.1 MCP Protocol Overview\n",
    "\n",
    "### Core MCP Concepts:\n",
    "1. **Tool Servers** - Services that expose capabilities via MCP protocol\n",
    "2. **Tool Registry** - Central catalog of available tools with schemas\n",
    "3. **Tool Invocation** - Standardized calling interface with validation\n",
    "4. **Streaming Results** - Real-time partial outputs for long-running tools\n",
    "5. **Error Handling** - Consistent error reporting and recovery\n",
    "\n",
    "### V2 MCP Architecture:\n",
    "- **Node.js Tools**: I/O intensive operations (API calls, file operations, messaging)\n",
    "- **Python Tools**: Computation heavy tasks (ML inference, data analysis, code execution)\n",
    "- **Bridge Layer**: Protocol translation between Node.js and Python ecosystems\n",
    "- **Schema Validation**: JSON Schema enforcement for all tool inputs/outputs\n",
    "- **Capability Discovery**: Dynamic tool registration and metadata exchange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeccbdef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: tools-python/src/mcp_server.py\n",
    "# Cartrita V2 Python MCP Server - ML and Computation Tools\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "import uuid\n",
    "import logging\n",
    "from typing import Dict, Any, List, Optional, AsyncGenerator, Callable\n",
    "from dataclasses import dataclass, asdict\n",
    "from datetime import datetime\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "import websockets\n",
    "from pydantic import BaseModel, ValidationError\n",
    "from opentelemetry import trace\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "tracer = trace.get_tracer(\"cartrita-v2-mcp-python\")\n",
    "\n",
    "@dataclass\n",
    "class ToolCapability:\n",
    "    \"\"\"Tool capability definition\"\"\"\n",
    "    name: str\n",
    "    description: str\n",
    "    parameters: Dict[str, Any]\n",
    "    returns: Dict[str, Any]\n",
    "    category: str\n",
    "    safety_level: str = \"safe\"\n",
    "    estimated_cost: int = 0\n",
    "    streaming: bool = False\n",
    "\n",
    "@dataclass\n",
    "class ToolInvocation:\n",
    "    \"\"\"Tool invocation request\"\"\"\n",
    "    id: str\n",
    "    tool_name: str\n",
    "    parameters: Dict[str, Any]\n",
    "    session_id: Optional[str] = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "\n",
    "@dataclass\n",
    "class ToolResult:\n",
    "    \"\"\"Tool execution result\"\"\"\n",
    "    id: str\n",
    "    success: bool\n",
    "    result: Any = None\n",
    "    error: Optional[str] = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "    streaming: bool = False\n",
    "\n",
    "class MCPTool(ABC):\n",
    "    \"\"\"Abstract base class for MCP tools\"\"\"\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def capability(self) -> ToolCapability:\n",
    "        \"\"\"Return tool capability definition\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    async def execute(self, parameters: Dict[str, Any], context: Dict[str, Any] = None) -> Any:\n",
    "        \"\"\"Execute the tool with given parameters\"\"\"\n",
    "        pass\n",
    "    \n",
    "    async def stream_execute(self, parameters: Dict[str, Any], context: Dict[str, Any] = None) -> AsyncGenerator[Any, None]:\n",
    "        \"\"\"Execute tool with streaming results (override for streaming tools)\"\"\"\n",
    "        result = await self.execute(parameters, context)\n",
    "        yield result\n",
    "\n",
    "class EmbeddingTool(MCPTool):\n",
    "    \"\"\"Generate embeddings for text using OpenAI\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        from openai import AsyncOpenAI\n",
    "        self.client = AsyncOpenAI()\n",
    "    \n",
    "    @property\n",
    "    def capability(self) -> ToolCapability:\n",
    "        return ToolCapability(\n",
    "            name=\"generate_embeddings\",\n",
    "            description=\"Generate vector embeddings for text using OpenAI\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"texts\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"},\n",
    "                        \"description\": \"List of texts to embed\"\n",
    "                    },\n",
    "                    \"model\": {\n",
    "                        \"type\": \"string\", \n",
    "                        \"default\": \"text-embedding-3-large\",\n",
    "                        \"description\": \"Embedding model to use\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"texts\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            returns={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"embeddings\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\n",
    "                            \"type\": \"array\",\n",
    "                            \"items\": {\"type\": \"number\"}\n",
    "                        }\n",
    "                    },\n",
    "                    \"model_used\": {\"type\": \"string\"},\n",
    "                    \"token_count\": {\"type\": \"integer\"}\n",
    "                }\n",
    "            },\n",
    "            category=\"ml\",\n",
    "            estimated_cost=10\n",
    "        )\n",
    "    \n",
    "    async def execute(self, parameters: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        with tracer.start_as_current_span(\"embedding-tool-execute\") as span:\n",
    "            texts = parameters[\"texts\"]\n",
    "            model = parameters.get(\"model\", \"text-embedding-3-large\")\n",
    "            \n",
    "            span.set_attributes({\n",
    "                \"text_count\": len(texts),\n",
    "                \"model\": model,\n",
    "                \"total_chars\": sum(len(t) for t in texts)\n",
    "            })\n",
    "            \n",
    "            try:\n",
    "                response = await self.client.embeddings.create(\n",
    "                    input=texts,\n",
    "                    model=model\n",
    "                )\n",
    "                \n",
    "                embeddings = [emb.embedding for emb in response.data]\n",
    "                \n",
    "                return {\n",
    "                    \"embeddings\": embeddings,\n",
    "                    \"model_used\": model,\n",
    "                    \"token_count\": response.usage.total_tokens\n",
    "                }\n",
    "                \n",
    "            except Exception as e:\n",
    "                span.record_exception(e)\n",
    "                raise\n",
    "\n",
    "class CodeExecutionTool(MCPTool):\n",
    "    \"\"\"Execute Python code in a sandboxed environment\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def capability(self) -> ToolCapability:\n",
    "        return ToolCapability(\n",
    "            name=\"execute_python\",\n",
    "            description=\"Execute Python code in a secure sandbox\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"code\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Python code to execute\"\n",
    "                    },\n",
    "                    \"timeout\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"default\": 30,\n",
    "                        \"description\": \"Execution timeout in seconds\"\n",
    "                    },\n",
    "                    \"packages\": {\n",
    "                        \"type\": \"array\",\n",
    "                        \"items\": {\"type\": \"string\"},\n",
    "                        \"description\": \"Required packages (pre-approved list only)\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"code\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            returns={\n",
    "                \"type\": \"object\", \n",
    "                \"properties\": {\n",
    "                    \"stdout\": {\"type\": \"string\"},\n",
    "                    \"stderr\": {\"type\": \"string\"},\n",
    "                    \"return_value\": {\"type\": \"string\"},\n",
    "                    \"execution_time\": {\"type\": \"number\"}\n",
    "                }\n",
    "            },\n",
    "            category=\"computation\",\n",
    "            safety_level=\"caution\",\n",
    "            estimated_cost=50,\n",
    "            streaming=True\n",
    "        )\n",
    "    \n",
    "    async def execute(self, parameters: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        import subprocess\n",
    "        import tempfile\n",
    "        import os\n",
    "        import time\n",
    "        \n",
    "        code = parameters[\"code\"]\n",
    "        timeout = parameters.get(\"timeout\", 30)\n",
    "        \n",
    "        # Security: Basic code validation\n",
    "        forbidden_imports = ['os', 'subprocess', 'sys', 'eval', 'exec', '__import__']\n",
    "        for forbidden in forbidden_imports:\n",
    "            if forbidden in code:\n",
    "                return {\n",
    "                    \"stdout\": \"\",\n",
    "                    \"stderr\": f\"Security error: '{forbidden}' not allowed\",\n",
    "                    \"return_value\": \"\",\n",
    "                    \"execution_time\": 0\n",
    "                }\n",
    "        \n",
    "        with tempfile.NamedTemporaryFile(mode='w', suffix='.py', delete=False) as f:\n",
    "            f.write(code)\n",
    "            temp_file = f.name\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            result = subprocess.run(\n",
    "                [\"python\", temp_file],\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=timeout\n",
    "            )\n",
    "            \n",
    "            execution_time = time.time() - start_time\n",
    "            \n",
    "            return {\n",
    "                \"stdout\": result.stdout,\n",
    "                \"stderr\": result.stderr,\n",
    "                \"return_value\": str(result.returncode),\n",
    "                \"execution_time\": execution_time\n",
    "            }\n",
    "            \n",
    "        except subprocess.TimeoutExpired:\n",
    "            return {\n",
    "                \"stdout\": \"\",\n",
    "                \"stderr\": f\"Execution timed out after {timeout}s\",\n",
    "                \"return_value\": \"\",\n",
    "                \"execution_time\": timeout\n",
    "            }\n",
    "        finally:\n",
    "            os.unlink(temp_file)\n",
    "\n",
    "class DocumentAnalysisTool(MCPTool):\n",
    "    \"\"\"Analyze documents for content extraction and summarization\"\"\"\n",
    "    \n",
    "    def __init__(self, openai_wrapper):\n",
    "        self.openai_wrapper = openai_wrapper\n",
    "    \n",
    "    @property \n",
    "    def capability(self) -> ToolCapability:\n",
    "        return ToolCapability(\n",
    "            name=\"analyze_document\",\n",
    "            description=\"Analyze and extract insights from document content\",\n",
    "            parameters={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"content\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"Document content to analyze\"\n",
    "                    },\n",
    "                    \"analysis_type\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"enum\": [\"summary\", \"extract_facts\", \"sentiment\", \"topics\"],\n",
    "                        \"description\": \"Type of analysis to perform\"\n",
    "                    },\n",
    "                    \"max_length\": {\n",
    "                        \"type\": \"integer\",\n",
    "                        \"default\": 500,\n",
    "                        \"description\": \"Maximum length of output\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"content\", \"analysis_type\"],\n",
    "                \"additionalProperties\": False\n",
    "            },\n",
    "            returns={\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"analysis\": {\"type\": \"string\"},\n",
    "                    \"confidence\": {\"type\": \"number\"},\n",
    "                    \"metadata\": {\"type\": \"object\"}\n",
    "                }\n",
    "            },\n",
    "            category=\"analysis\",\n",
    "            estimated_cost=100\n",
    "        )\n",
    "    \n",
    "    async def execute(self, parameters: Dict[str, Any], context: Dict[str, Any] = None) -> Dict[str, Any]:\n",
    "        from langchain.schema import SystemMessage, HumanMessage\n",
    "        \n",
    "        content = parameters[\"content\"]\n",
    "        analysis_type = parameters[\"analysis_type\"]\n",
    "        max_length = parameters.get(\"max_length\", 500)\n",
    "        \n",
    "        # Create analysis prompt based on type\n",
    "        prompts = {\n",
    "            \"summary\": f\"Provide a concise summary of the following content in no more than {max_length} words:\",\n",
    "            \"extract_facts\": f\"Extract key facts and data points from the following content (max {max_length} words):\",\n",
    "            \"sentiment\": f\"Analyze the sentiment of the following content and provide a brief explanation (max {max_length} words):\",\n",
    "            \"topics\": f\"Identify the main topics and themes in the following content (max {max_length} words):\"\n",
    "        }\n",
    "        \n",
    "        system_prompt = prompts[analysis_type]\n",
    "        \n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=content)\n",
    "        ]\n",
    "        \n",
    "        response = await self.openai_wrapper.complete(\n",
    "            messages=messages,\n",
    "            session_id=context.get(\"session_id\") if context else None\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"analysis\": response.content,\n",
    "            \"confidence\": 0.85,  # Placeholder - could implement confidence scoring\n",
    "            \"metadata\": {\n",
    "                \"analysis_type\": analysis_type,\n",
    "                \"content_length\": len(content),\n",
    "                \"output_length\": len(response.content)\n",
    "            }\n",
    "        }\n",
    "\n",
    "class MCPServer:\n",
    "    \"\"\"MCP Server for Python tools\"\"\"\n",
    "    \n",
    "    def __init__(self, host: str = \"localhost\", port: int = 8001):\n",
    "        self.host = host\n",
    "        self.port = port\n",
    "        self.tools: Dict[str, MCPTool] = {}\n",
    "        self.active_connections: Dict[str, websockets.WebSocketServerProtocol] = {}\n",
    "        \n",
    "    def register_tool(self, tool: MCPTool):\n",
    "        \"\"\"Register a tool with the server\"\"\"\n",
    "        self.tools[tool.capability.name] = tool\n",
    "        logger.info(f\"Registered tool: {tool.capability.name}\")\n",
    "    \n",
    "    async def start(self):\n",
    "        \"\"\"Start the MCP server\"\"\"\n",
    "        logger.info(f\"Starting Cartrita V2 Python MCP Server on {self.host}:{self.port}\")\n",
    "        \n",
    "        async with websockets.serve(self.handle_client, self.host, self.port):\n",
    "            logger.info(\"üêç Python MCP Server ready for tool invocations!\")\n",
    "            await asyncio.Future()  # Keep server running\n",
    "    \n",
    "    async def handle_client(self, websocket, path):\n",
    "        \"\"\"Handle client connection\"\"\"\n",
    "        client_id = str(uuid.uuid4())\n",
    "        self.active_connections[client_id] = websocket\n",
    "        \n",
    "        logger.info(f\"Client connected: {client_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Send capabilities on connect\n",
    "            await self.send_capabilities(websocket)\n",
    "            \n",
    "            async for message in websocket:\n",
    "                try:\n",
    "                    data = json.loads(message)\n",
    "                    await self.process_message(websocket, data)\n",
    "                except json.JSONDecodeError:\n",
    "                    await self.send_error(websocket, \"Invalid JSON\", \"parse_error\")\n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error processing message: {e}\")\n",
    "                    await self.send_error(websocket, str(e), \"processing_error\")\n",
    "                    \n",
    "        except websockets.exceptions.ConnectionClosed:\n",
    "            logger.info(f\"Client disconnected: {client_id}\")\n",
    "        finally:\n",
    "            self.active_connections.pop(client_id, None)\n",
    "    \n",
    "    async def send_capabilities(self, websocket):\n",
    "        \"\"\"Send tool capabilities to client\"\"\"\n",
    "        capabilities = {\n",
    "            \"type\": \"capabilities\",\n",
    "            \"tools\": [asdict(tool.capability) for tool in self.tools.values()],\n",
    "            \"server_info\": {\n",
    "                \"name\": \"cartrita-v2-python-tools\",\n",
    "                \"version\": \"2.0.0\",\n",
    "                \"description\": \"Python ML and computation tools\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        await websocket.send(json.dumps(capabilities))\n",
    "    \n",
    "    async def process_message(self, websocket, data: Dict[str, Any]):\n",
    "        \"\"\"Process incoming message\"\"\"\n",
    "        msg_type = data.get(\"type\")\n",
    "        \n",
    "        if msg_type == \"tool_call\":\n",
    "            await self.handle_tool_call(websocket, data)\n",
    "        elif msg_type == \"ping\":\n",
    "            await websocket.send(json.dumps({\"type\": \"pong\"}))\n",
    "        else:\n",
    "            await self.send_error(websocket, f\"Unknown message type: {msg_type}\", \"unknown_type\")\n",
    "    \n",
    "    async def handle_tool_call(self, websocket, data: Dict[str, Any]):\n",
    "        \"\"\"Handle tool invocation\"\"\"\n",
    "        with tracer.start_as_current_span(\"mcp-tool-call\") as span:\n",
    "            try:\n",
    "                invocation = ToolInvocation(\n",
    "                    id=data[\"id\"],\n",
    "                    tool_name=data[\"tool_name\"],\n",
    "                    parameters=data[\"parameters\"],\n",
    "                    session_id=data.get(\"session_id\"),\n",
    "                    metadata=data.get(\"metadata\", {})\n",
    "                )\n",
    "                \n",
    "                span.set_attributes({\n",
    "                    \"tool_name\": invocation.tool_name,\n",
    "                    \"invocation_id\": invocation.id,\n",
    "                    \"session_id\": invocation.session_id or \"unknown\"\n",
    "                })\n",
    "                \n",
    "                if invocation.tool_name not in self.tools:\n",
    "                    await self.send_error(websocket, f\"Unknown tool: {invocation.tool_name}\", \"unknown_tool\", invocation.id)\n",
    "                    return\n",
    "                \n",
    "                tool = self.tools[invocation.tool_name]\n",
    "                \n",
    "                # Validate parameters against schema\n",
    "                # TODO: Add JSON schema validation\n",
    "                \n",
    "                if tool.capability.streaming:\n",
    "                    await self.handle_streaming_tool(websocket, tool, invocation)\n",
    "                else:\n",
    "                    await self.handle_standard_tool(websocket, tool, invocation)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                span.record_exception(e)\n",
    "                logger.error(f\"Tool execution error: {e}\")\n",
    "                await self.send_error(websocket, str(e), \"execution_error\", data.get(\"id\"))\n",
    "    \n",
    "    async def handle_standard_tool(self, websocket, tool: MCPTool, invocation: ToolInvocation):\n",
    "        \"\"\"Handle standard (non-streaming) tool execution\"\"\"\n",
    "        try:\n",
    "            context = {\n",
    "                \"session_id\": invocation.session_id,\n",
    "                \"invocation_id\": invocation.id,\n",
    "                \"metadata\": invocation.metadata\n",
    "            }\n",
    "            \n",
    "            result = await tool.execute(invocation.parameters, context)\n",
    "            \n",
    "            response = ToolResult(\n",
    "                id=invocation.id,\n",
    "                success=True,\n",
    "                result=result\n",
    "            )\n",
    "            \n",
    "            await websocket.send(json.dumps({\n",
    "                \"type\": \"tool_result\",\n",
    "                **asdict(response)\n",
    "            }))\n",
    "            \n",
    "        except Exception as e:\n",
    "            await self.send_error(websocket, str(e), \"execution_error\", invocation.id)\n",
    "    \n",
    "    async def handle_streaming_tool(self, websocket, tool: MCPTool, invocation: ToolInvocation):\n",
    "        \"\"\"Handle streaming tool execution\"\"\"\n",
    "        try:\n",
    "            context = {\n",
    "                \"session_id\": invocation.session_id,\n",
    "                \"invocation_id\": invocation.id,\n",
    "                \"metadata\": invocation.metadata\n",
    "            }\n",
    "            \n",
    "            async for chunk in tool.stream_execute(invocation.parameters, context):\n",
    "                await websocket.send(json.dumps({\n",
    "                    \"type\": \"tool_chunk\",\n",
    "                    \"id\": invocation.id,\n",
    "                    \"chunk\": chunk,\n",
    "                    \"streaming\": True\n",
    "                }))\n",
    "            \n",
    "            # Send completion marker\n",
    "            await websocket.send(json.dumps({\n",
    "                \"type\": \"tool_complete\",\n",
    "                \"id\": invocation.id,\n",
    "                \"success\": True\n",
    "            }))\n",
    "            \n",
    "        except Exception as e:\n",
    "            await self.send_error(websocket, str(e), \"streaming_error\", invocation.id)\n",
    "    \n",
    "    async def send_error(self, websocket, message: str, error_type: str, invocation_id: str = None):\n",
    "        \"\"\"Send error response\"\"\"\n",
    "        error_data = {\n",
    "            \"type\": \"error\",\n",
    "            \"error_type\": error_type,\n",
    "            \"message\": message\n",
    "        }\n",
    "        \n",
    "        if invocation_id:\n",
    "            error_data[\"id\"] = invocation_id\n",
    "        \n",
    "        await websocket.send(json.dumps(error_data))\n",
    "\n",
    "async def main():\n",
    "    \"\"\"Start the MCP server with registered tools\"\"\"\n",
    "    server = MCPServer()\n",
    "    \n",
    "    # Register tools\n",
    "    server.register_tool(EmbeddingTool())\n",
    "    server.register_tool(CodeExecutionTool())\n",
    "    \n",
    "    # Would need OpenAI wrapper instance for this\n",
    "    # from shared.llm.openai_wrapper import create_execution_wrapper\n",
    "    # openai_wrapper = create_execution_wrapper()\n",
    "    # server.register_tool(DocumentAnalysisTool(openai_wrapper))\n",
    "    \n",
    "    await server.start()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    asyncio.run(main())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
